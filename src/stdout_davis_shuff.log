Sender: LSF System <lsfadmin@lsf-server-3.rostclust>
Subject: Job 13219: <dg_prediction_davis_shuff> in cluster <rost_lsf_cluster_1> Done

Job <dg_prediction_davis_shuff> was submitted from host <lsf-master-1.rostclust> by user <giessing> in cluster <rost_lsf_cluster_1> at Mon Dec 20 09:36:12 2021
Job was executed on host(s) <lsf-server-3.rostclust>, in queue <low-end-normal>, as user <giessing> in cluster <rost_lsf_cluster_1> at Mon Dec 20 09:36:12 2021
</mnt/home/giessing> was used as the home directory.
</mnt/project/protdrugaffinity/dg/binding-affinity-prediction/src> was used as the working directory.
Started at Mon Dec 20 09:36:12 2021
Terminated at Tue Dec 21 15:30:16 2021
Results reported at Tue Dec 21 15:30:16 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python binding_prediction.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   241992.83 sec.
    Max Memory :                                 1759 MB
    Average Memory :                             1705.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                26
    Run time :                                   107643 sec.
    Turnaround time :                            107644 sec.

The output (if any) follows:

(['pkd', 'Davis'], 'chemVAE', <Section: INPUT FILES>, True, 1, True, True)
Using device: cuda
[720, 0.008, 220]
[720, 0.008, 220]
[690, 0.0077, 186]
[760, 0.0013000000000000002, 142]
[760, 0.0013000000000000002, 142]
[520, 0.0084, 214]
[195, 0.0012000000000000001, 145]
[195, 0.0012000000000000001, 145]
[825, 0.0035, 245]
[655, 0.0048000000000000004, 285]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
[520, 0.0041, 222]
Finished Tuning
0.41872614217069115
[520, 0.0041, 222]
[1] loss: 852.5664941
[2] loss: 44.7792520
[3] loss: 31.7202688
[4] loss: 29.8188072
[5] loss: 29.0799326
[6] loss: 29.1144202
[7] loss: 28.6419827
[8] loss: 28.4097335
[9] loss: 28.0784351
[10] loss: 28.1852697
[11] loss: 27.8336674
[12] loss: 27.7941687
[13] loss: 27.3846978
[14] loss: 27.4825896
[15] loss: 27.3497065
[16] loss: 27.1818112
[17] loss: 27.1638752
[18] loss: 27.1373801
[19] loss: 26.7333335
[20] loss: 26.6379146
[21] loss: 26.4802282
[22] loss: 26.3618194
[23] loss: 26.3160714
[24] loss: 26.2183961
[25] loss: 26.1970314
[26] loss: 26.1066718
[27] loss: 25.9508539
[28] loss: 25.9171590
[29] loss: 25.8977893
[30] loss: 25.8643425
[31] loss: 25.8985233
[32] loss: 25.8318773
[33] loss: 25.8679835
[34] loss: 25.8465198
[35] loss: 25.8603998
[36] loss: 25.9489822
[37] loss: 26.0339093
[38] loss: 26.0280126
[39] loss: 25.8898448
[40] loss: 25.7473754
[41] loss: 25.7404226
[42] loss: 25.7903961
[43] loss: 25.8023703
[44] loss: 25.8709845
[45] loss: 25.7824349
[46] loss: 25.7791925
[47] loss: 25.7394042
[48] loss: 25.7375546
[49] loss: 25.7436311
[50] loss: 25.7055137
[51] loss: 25.7101209
[52] loss: 25.7469122
[53] loss: 25.7627526
[54] loss: 25.8024546
[55] loss: 25.8104126
[56] loss: 25.9063681
[57] loss: 25.7857475
[58] loss: 25.7459174
[59] loss: 25.7249632
[60] loss: 25.7792037
[61] loss: 25.7637557
[62] loss: 25.7505600
[63] loss: 25.7342244
[64] loss: 25.7917677
[65] loss: 25.7884597
[66] loss: 25.7236081
[67] loss: 25.7588913
[68] loss: 25.6980551
[69] loss: 25.7056084
[70] loss: 25.7519222
[71] loss: 25.7303400
[72] loss: 25.7421653
[73] loss: 25.7010911
[74] loss: 25.7122208
[75] loss: 25.7085988
[76] loss: 25.7422155
[77] loss: 25.7918668
[78] loss: 25.6946665
[79] loss: 25.7322151
[80] loss: 25.7677377
[81] loss: 25.7549388
[82] loss: 25.7174863
[83] loss: 25.7890188
[84] loss: 25.7342751
[85] loss: 25.7291305
[86] loss: 25.6697512
[87] loss: 25.7039529
[88] loss: 25.6793334
[89] loss: 25.6935368
[90] loss: 25.7055978
[91] loss: 25.7739306
[92] loss: 25.7256413
[93] loss: 25.7926647
[94] loss: 25.7216082
[95] loss: 25.7207835
[96] loss: 25.7598542
[97] loss: 25.9792683
[98] loss: 25.7940139
[99] loss: 25.7576429
[100] loss: 25.7540566
[101] loss: 25.7089580
[102] loss: 25.7221136
[103] loss: 25.8036928
[104] loss: 25.7563950
[105] loss: 25.7128612
[106] loss: 25.7693873
[107] loss: 25.7126575
[108] loss: 25.7140166
[109] loss: 25.7305212
[110] loss: 25.7221886
[111] loss: 25.6936609
[112] loss: 25.7510832
[113] loss: 25.7499007
[114] loss: 25.7593615
[115] loss: 25.7006388
[116] loss: 25.6882950
[117] loss: 25.7769108
[118] loss: 25.7506414
[119] loss: 25.7135612
[120] loss: 25.7505748
[121] loss: 25.8382867
[122] loss: 25.8182788
[123] loss: 25.8551974
[124] loss: 25.9775467
[125] loss: 25.8056576
[126] loss: 25.8323571
[127] loss: 25.8034661
[128] loss: 25.8011347
[129] loss: 25.8021701
[130] loss: 25.8286315
[131] loss: 25.8128275
[132] loss: 25.7291724
[133] loss: 25.7466547
[134] loss: 25.8006520
[135] loss: 25.8430348
[136] loss: 25.8023594
[137] loss: 25.7365671
[138] loss: 25.8681839
[139] loss: 25.7720814
[140] loss: 25.7348634
[141] loss: 25.8189442
[142] loss: 25.8500185
[143] loss: 25.7649657
[144] loss: 25.8049276
[145] loss: 25.7474588
[146] loss: 25.8585225
[147] loss: 25.8433942
[148] loss: 25.8089694
[149] loss: 25.8389105
[150] loss: 25.8072985
[151] loss: 25.7536751
[152] loss: 25.7847680
[153] loss: 25.7544286
[154] loss: 25.7902896
[155] loss: 25.8058503
[156] loss: 25.7792509
[157] loss: 25.7582312
[158] loss: 25.7776893
[159] loss: 25.7850740
[160] loss: 25.7938733
[161] loss: 25.9011743
[162] loss: 25.8215984
[163] loss: 25.9309850
[164] loss: 25.8746926
[165] loss: 25.8350192
[166] loss: 25.8436782
[167] loss: 25.8116814
[168] loss: 25.9527009
[169] loss: 25.8052367
[170] loss: 25.9129250
[171] loss: 25.8878225
[172] loss: 25.7137069
[173] loss: 25.9151086
[174] loss: 25.9032709
[175] loss: 25.8968331
[176] loss: 25.8993455
[177] loss: 25.7961375
[178] loss: 25.8844012
[179] loss: 25.8289649
[180] loss: 25.9572537
[181] loss: 25.9893069
[182] loss: 25.9249851
[183] loss: 26.0273798
[184] loss: 25.9818007
[185] loss: 25.8919875
[186] loss: 26.0130828
[187] loss: 25.9539965
[188] loss: 25.9329146
[189] loss: 25.9499914
[190] loss: 25.9137162
[191] loss: 25.8692486
[192] loss: 25.9247980
[193] loss: 25.8781389
[194] loss: 26.0003581
[195] loss: 26.1473137
[196] loss: 26.3053087
[197] loss: 25.9101641
[198] loss: 26.0444373
[199] loss: 25.9615669
[200] loss: 26.0944485
[201] loss: 25.8198808
[202] loss: 26.0289706
[203] loss: 25.9509007
[204] loss: 25.9579050
[205] loss: 25.9937957
[206] loss: 25.9194221
[207] loss: 25.9561101
[208] loss: 26.0206599
[209] loss: 26.0163621
[210] loss: 26.0055564
[211] loss: 25.8222393
[212] loss: 25.9601336
[213] loss: 25.9505904
[214] loss: 25.9293536
[215] loss: 25.9017710
[216] loss: 25.8656776
[217] loss: 25.9682844
[218] loss: 25.8808379
[219] loss: 25.9598358
[220] loss: 26.0095784
[221] loss: 25.9280798
[222] loss: 25.9720486
Finished Training
The r2m value for this run is:  0.226
The AUPR for this run is:  0.386
The Concordance Index (CI) for this run is:  0.728
The Mean Squared Error (MSE) for this run is:  0.63
r2m std is:  0.006
AUPR std is:  0.012
CIs std is:  0.003
Best r2m was:  0.41872614217069115
Best parameters were: [520, 0.0041, 222]


PS:

Read file <stderr_davis_shuff.log> for stderr output of this job.

