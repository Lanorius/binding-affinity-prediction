Sender: LSF System <lsfadmin@lsf-server-3.rostclust>
Subject: Job 14963: <dg_prediction_davis_d08_t09> in cluster <rost_lsf_cluster_1> Done

Job <dg_prediction_davis_d08_t09> was submitted from host <lsf-master-1.rostclust> by user <giessing> in cluster <rost_lsf_cluster_1> at Fri Feb 11 09:30:06 2022
Job was executed on host(s) <lsf-server-3.rostclust>, in queue <low-end-normal>, as user <giessing> in cluster <rost_lsf_cluster_1> at Fri Feb 11 09:30:06 2022
</mnt/home/giessing> was used as the home directory.
</mnt/project/protdrugaffinity/dg/binding-affinity-prediction/src> was used as the working directory.
Started at Fri Feb 11 09:30:06 2022
Terminated at Sat Feb 12 03:52:34 2022
Results reported at Sat Feb 12 03:52:34 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python binding_prediction.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   256117.92 sec.
    Max Memory :                                 1691 MB
    Average Memory :                             1679.43 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                28
    Run time :                                   66147 sec.
    Turnaround time :                            66148 sec.

The output (if any) follows:

Using device: cuda
Finished Tuning
[500, 0.0001, 365]
Training attempt No: 1
[1] loss: 8.2480984
[2] loss: 3.8264681
[3] loss: 3.7879784
[4] loss: 3.7796911
[5] loss: 3.7146790
[6] loss: 3.7444496
[7] loss: 3.7399652
[8] loss: 3.7265392
[9] loss: 3.6930934
[10] loss: 3.6545291
[11] loss: 3.6471145
[12] loss: 3.5905590
[13] loss: 3.5610810
[14] loss: 3.4994436
[15] loss: 3.5117483
[16] loss: 3.4618482
[17] loss: 3.4807126
[18] loss: 3.4538712
[19] loss: 3.4624860
[20] loss: 3.4113292
[21] loss: 3.4116092
[22] loss: 3.4028776
[23] loss: 3.3638935
[24] loss: 3.3383983
[25] loss: 3.3454637
[26] loss: 3.3527287
[27] loss: 3.3184046
[28] loss: 3.2925985
[29] loss: 3.2794245
[30] loss: 3.2700888
[31] loss: 3.2629621
[32] loss: 3.2391720
[33] loss: 3.2066839
[34] loss: 3.2006517
[35] loss: 3.1244482
[36] loss: 3.1506015
[37] loss: 3.1514562
[38] loss: 3.1563783
[39] loss: 3.0798899
[40] loss: 3.0780849
[41] loss: 3.0720092
[42] loss: 3.0516470
[43] loss: 3.0612911
[44] loss: 3.0034890
[45] loss: 3.0060713
[46] loss: 2.9980438
[47] loss: 3.0134563
[48] loss: 2.9607096
[49] loss: 2.9782928
[50] loss: 2.9729811
[51] loss: 3.0007583
[52] loss: 2.9578562
[53] loss: 2.9821917
[54] loss: 2.9593907
[55] loss: 2.9259051
[56] loss: 2.9139343
[57] loss: 2.9530741
[58] loss: 2.9585918
[59] loss: 2.8954857
[60] loss: 3.0004355
[61] loss: 2.9406924
[62] loss: 2.9212526
[63] loss: 2.9156919
[64] loss: 2.9175611
[65] loss: 2.9395685
[66] loss: 2.9256516
[67] loss: 2.9295195
[68] loss: 2.8908805
[69] loss: 2.8908548
[70] loss: 2.8463875
[71] loss: 2.9102385
[72] loss: 2.8443664
[73] loss: 2.8161963
[74] loss: 2.8751877
[75] loss: 2.8539294
[76] loss: 2.8455095
[77] loss: 2.8797878
[78] loss: 2.8527489
[79] loss: 2.8668709
[80] loss: 2.8272148
[81] loss: 2.8187360
[82] loss: 2.8295138
[83] loss: 2.8379144
[84] loss: 2.8092633
[85] loss: 2.8459219
[86] loss: 2.8060309
[87] loss: 2.7565799
[88] loss: 2.7382188
[89] loss: 2.7382559
[90] loss: 2.7735705
[91] loss: 2.7417293
[92] loss: 2.7306290
[93] loss: 2.7479261
[94] loss: 2.7697049
[95] loss: 2.7425673
[96] loss: 2.7759473
[97] loss: 2.7241491
[98] loss: 2.7251102
[99] loss: 2.7269749
[100] loss: 2.7224173
[101] loss: 2.7325570
[102] loss: 2.7076455
[103] loss: 2.7080852
[104] loss: 2.6877827
[105] loss: 2.7327270
[106] loss: 2.6396233
[107] loss: 2.6825559
[108] loss: 2.7021963
[109] loss: 2.6535804
[110] loss: 2.6308739
[111] loss: 2.6633197
[112] loss: 2.6009269
[113] loss: 2.6006413
[114] loss: 2.6308709
[115] loss: 2.5738827
[116] loss: 2.5917091
[117] loss: 2.6178604
[118] loss: 2.6105830
[119] loss: 2.6135747
[120] loss: 2.5894465
[121] loss: 2.6194861
[122] loss: 2.6000484
[123] loss: 2.5448314
[124] loss: 2.6136111
[125] loss: 2.5798899
[126] loss: 2.6123560
[127] loss: 2.5281228
[128] loss: 2.6270277
[129] loss: 2.5412252
[130] loss: 2.5699468
[131] loss: 2.5436094
[132] loss: 2.5641868
[133] loss: 2.5434272
[134] loss: 2.5139014
[135] loss: 2.5042817
[136] loss: 2.5025785
[137] loss: 2.5014435
[138] loss: 2.5136972
[139] loss: 2.5329682
[140] loss: 2.4856240
[141] loss: 2.5547849
[142] loss: 2.5183998
[143] loss: 2.5391526
[144] loss: 2.5252353
[145] loss: 2.5025861
[146] loss: 2.4986962
[147] loss: 2.4782636
[148] loss: 2.5156375
[149] loss: 2.4427314
[150] loss: 2.4979396
[151] loss: 2.4736492
[152] loss: 2.4684347
[153] loss: 2.4719617
[154] loss: 2.4575553
[155] loss: 2.4501681
[156] loss: 2.4731748
[157] loss: 2.4748657
[158] loss: 2.4993229
[159] loss: 2.4278707
[160] loss: 2.4486627
[161] loss: 2.4399712
[162] loss: 2.4663779
[163] loss: 2.4097904
[164] loss: 2.4614859
[165] loss: 2.4567412
[166] loss: 2.4395033
[167] loss: 2.4145045
[168] loss: 2.4486393
[169] loss: 2.4291669
[170] loss: 2.4779023
[171] loss: 2.4640225
[172] loss: 2.4079370
[173] loss: 2.4062359
[174] loss: 2.3793904
[175] loss: 2.4007901
[176] loss: 2.4318475
[177] loss: 2.4266181
[178] loss: 2.3921298
[179] loss: 2.4670225
[180] loss: 2.3950952
[181] loss: 2.3903929
[182] loss: 2.4159477
[183] loss: 2.4010626
[184] loss: 2.4056782
[185] loss: 2.3511200
[186] loss: 2.3774759
[187] loss: 2.3791595
[188] loss: 2.3504160
[189] loss: 2.3333829
[190] loss: 2.3726506
[191] loss: 2.4028527
[192] loss: 2.3686098
[193] loss: 2.3925772
[194] loss: 2.3566539
[195] loss: 2.3360281
[196] loss: 2.3689738
[197] loss: 2.3455668
[198] loss: 2.3791552
[199] loss: 2.3576596
[200] loss: 2.3348530
[201] loss: 2.3839332
[202] loss: 2.3424610
[203] loss: 2.3312964
[204] loss: 2.4054868
[205] loss: 2.3300397
[206] loss: 2.3237800
[207] loss: 2.3576035
[208] loss: 2.3338966
[209] loss: 2.3115364
[210] loss: 2.2941954
[211] loss: 2.2918863
[212] loss: 2.2912907
[213] loss: 2.3358518
[214] loss: 2.3043462
[215] loss: 2.3063598
[216] loss: 2.3591985
[217] loss: 2.2871860
[218] loss: 2.3470768
[219] loss: 2.3218534
[220] loss: 2.3140818
[221] loss: 2.3055316
[222] loss: 2.3076769
[223] loss: 2.2933023
[224] loss: 2.2665770
[225] loss: 2.2739190
[226] loss: 2.3575262
[227] loss: 2.2721495
[228] loss: 2.2846861
[229] loss: 2.3271342
[230] loss: 2.2924117
[231] loss: 2.2365695
[232] loss: 2.3133639
[233] loss: 2.2396220
[234] loss: 2.2481255
[235] loss: 2.2481035
[236] loss: 2.3144032
[237] loss: 2.2380207
[238] loss: 2.3027085
[239] loss: 2.3074743
[240] loss: 2.2709053
[241] loss: 2.2439349
[242] loss: 2.2674191
[243] loss: 2.2633501
[244] loss: 2.2388452
[245] loss: 2.2798584
[246] loss: 2.2519999
[247] loss: 2.2696206
[248] loss: 2.2747867
[249] loss: 2.2835970
[250] loss: 2.2536437
[251] loss: 2.2417281
[252] loss: 2.2055045
[253] loss: 2.2120342
[254] loss: 2.2231148
[255] loss: 2.2462488
[256] loss: 2.2216164
[257] loss: 2.1711086
[258] loss: 2.1643450
[259] loss: 2.2170816
[260] loss: 2.2141505
[261] loss: 2.2293550
[262] loss: 2.2449406
[263] loss: 2.2159320
[264] loss: 2.2215538
[265] loss: 2.2104369
[266] loss: 2.1900210
[267] loss: 2.2049890
[268] loss: 2.2274058
[269] loss: 2.1932659
[270] loss: 2.1925674
[271] loss: 2.1779760
[272] loss: 2.1888864
[273] loss: 2.1957377
[274] loss: 2.2348103
[275] loss: 2.2304948
[276] loss: 2.1790295
[277] loss: 2.2286866
[278] loss: 2.2105465
[279] loss: 2.2058344
[280] loss: 2.1878621
[281] loss: 2.1763108
[282] loss: 2.1875960
[283] loss: 2.1836065
[284] loss: 2.1857114
[285] loss: 2.1769915
[286] loss: 2.1664366
[287] loss: 2.1305206
[288] loss: 2.1891551
[289] loss: 2.1603685
[290] loss: 2.1588884
[291] loss: 2.2093608
[292] loss: 2.1403377
[293] loss: 2.2007197
[294] loss: 2.1467308
[295] loss: 2.1909495
[296] loss: 2.1267780
[297] loss: 2.1843306
[298] loss: 2.1493846
[299] loss: 2.1720892
[300] loss: 2.1864315
[301] loss: 2.1918566
[302] loss: 2.1196233
[303] loss: 2.1376160
[304] loss: 2.1396433
[305] loss: 2.0920624
[306] loss: 2.1719045
[307] loss: 2.1453344
[308] loss: 2.0604200
[309] loss: 2.1265103
[310] loss: 2.1451273
[311] loss: 2.1295191
[312] loss: 2.1576467
[313] loss: 2.0842337
[314] loss: 2.1244985
[315] loss: 2.1350841
[316] loss: 2.1289889
[317] loss: 2.1095372
[318] loss: 2.2393193
[319] loss: 2.0769845
[320] loss: 2.1085470
[321] loss: 2.0773437
[322] loss: 2.0842978
[323] loss: 2.0953053
[324] loss: 2.1399183
[325] loss: 2.0503905
[326] loss: 2.0904196
[327] loss: 2.1004242
[328] loss: 2.0893835
[329] loss: 2.0657870
[330] loss: 2.1367617
[331] loss: 2.1017305
[332] loss: 2.1170365
[333] loss: 2.1368099
[334] loss: 2.0754637
[335] loss: 2.1254619
[336] loss: 2.0926528
[337] loss: 2.0826430
[338] loss: 2.1360113
[339] loss: 2.1131236
[340] loss: 2.1101795
[341] loss: 2.1092846
[342] loss: 2.0508471
[343] loss: 2.1104418
[344] loss: 2.0783740
[345] loss: 2.0551289
[346] loss: 2.0776323
[347] loss: 2.1047293
[348] loss: 2.0532812
[349] loss: 2.0520527
[350] loss: 2.0742768
[351] loss: 2.0728403
[352] loss: 2.0896379
[353] loss: 2.0796832
[354] loss: 2.0666819
[355] loss: 2.0833251
[356] loss: 2.1533276
[357] loss: 2.0727308
[358] loss: 2.0684374
[359] loss: 2.1062363
[360] loss: 2.0899594
[361] loss: 2.0848240
[362] loss: 2.0713883
[363] loss: 2.0830850
[364] loss: 2.0853297
[365] loss: 2.0931347
Finished Training
The r2m value for this run is:  0.545
The AUPR for this run is:  0.632
The Concordance Index (CI) for this run is:  0.849
The Mean Squared Error (MSE) for this run is:  0.276
r2m std is:  0.014
AUPR std is:  0.022
CI std is:  0.004
MSE std is:  0.008
Best parameters were: [500, 0.0001, 365]


PS:

Read file <stderr_davis_d08_t09.log> for stderr output of this job.

