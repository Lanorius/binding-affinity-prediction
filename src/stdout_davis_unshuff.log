Sender: LSF System <lsfadmin@lsf-server-3.rostclust>
Subject: Job 13430: <dg_prediction_davis_unshuff> in cluster <rost_lsf_cluster_1> Done

Job <dg_prediction_davis_unshuff> was submitted from host <lsf-master-1.rostclust> by user <giessing> in cluster <rost_lsf_cluster_1> at Thu Dec 30 09:44:13 2021
Job was executed on host(s) <lsf-server-3.rostclust>, in queue <low-end-normal>, as user <giessing> in cluster <rost_lsf_cluster_1> at Thu Dec 30 09:44:14 2021
</mnt/home/giessing> was used as the home directory.
</mnt/project/protdrugaffinity/dg/binding-affinity-prediction/src> was used as the working directory.
Started at Thu Dec 30 09:44:14 2021
Terminated at Sat Jan  1 05:32:31 2022
Results reported at Sat Jan  1 05:32:31 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python binding_prediction.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   423205.84 sec.
    Max Memory :                                 1645 MB
    Average Memory :                             436.89 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                26
    Run time :                                   157696 sec.
    Turnaround time :                            157698 sec.

The output (if any) follows:

(['pkd', 'Davis'], 'chemVAE', <Section: INPUT FILES>, True, 1, False, False)
Using device: cuda
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
[20, 0.001, 281]
Finished Tuning
0.6363491128121634
[20, 0.001, 281]
[1] loss: 1014.9531506
[2] loss: 773.6417382
[3] loss: 706.2414257
[4] loss: 675.0275783
[5] loss: 651.9389370
[6] loss: 623.1767799
[7] loss: 604.0748881
[8] loss: 573.7970157
[9] loss: 560.9257944
[10] loss: 551.0688746
[11] loss: 531.8308296
[12] loss: 521.8771651
[13] loss: 508.9393720
[14] loss: 503.5993893
[15] loss: 491.3904586
[16] loss: 489.0171477
[17] loss: 485.4493431
[18] loss: 475.4395964
[19] loss: 469.6156626
[20] loss: 465.4492324
[21] loss: 457.9819386
[22] loss: 447.9485925
[23] loss: 444.8293584
[24] loss: 438.1910975
[25] loss: 433.0915318
[26] loss: 426.2632605
[27] loss: 425.5842683
[28] loss: 421.2053604
[29] loss: 412.7222364
[30] loss: 410.3948329
[31] loss: 406.8809295
[32] loss: 403.3595512
[33] loss: 397.5945498
[34] loss: 395.8722433
[35] loss: 393.1170426
[36] loss: 388.7794557
[37] loss: 388.4764663
[38] loss: 383.2882536
[39] loss: 380.0685583
[40] loss: 379.3404074
[41] loss: 374.8030252
[42] loss: 371.5879141
[43] loss: 367.2176372
[44] loss: 363.9780540
[45] loss: 361.4557647
[46] loss: 360.1409229
[47] loss: 356.8490449
[48] loss: 354.1134400
[49] loss: 353.1471532
[50] loss: 349.3133706
[51] loss: 345.2732663
[52] loss: 347.2438590
[53] loss: 342.4677713
[54] loss: 340.5292602
[55] loss: 338.8260190
[56] loss: 335.0419046
[57] loss: 331.4038831
[58] loss: 328.5508011
[59] loss: 331.0328446
[60] loss: 327.1814142
[61] loss: 325.8738383
[62] loss: 324.3600360
[63] loss: 319.9037893
[64] loss: 315.7540002
[65] loss: 315.7584941
[66] loss: 311.2655839
[67] loss: 309.1514947
[68] loss: 306.7909631
[69] loss: 305.2848061
[70] loss: 306.8047154
[71] loss: 303.0849826
[72] loss: 300.8031631
[73] loss: 296.7548773
[74] loss: 296.5950113
[75] loss: 294.8555084
[76] loss: 291.8349360
[77] loss: 289.6433311
[78] loss: 290.8363076
[79] loss: 286.9506754
[80] loss: 287.1328659
[81] loss: 283.0138436
[82] loss: 280.1229513
[83] loss: 281.7180676
[84] loss: 278.7507597
[85] loss: 282.0049491
[86] loss: 276.3724071
[87] loss: 275.8329428
[88] loss: 276.0634306
[89] loss: 271.8372950
[90] loss: 269.9968430
[91] loss: 267.7268718
[92] loss: 267.9313602
[93] loss: 264.8228620
[94] loss: 264.0306506
[95] loss: 262.4978800
[96] loss: 261.0452194
[97] loss: 260.6645147
[98] loss: 258.2012855
[99] loss: 255.1371091
[100] loss: 255.7707134
[101] loss: 253.8890455
[102] loss: 250.7705501
[103] loss: 250.2863744
[104] loss: 248.3210065
[105] loss: 248.6326092
[106] loss: 246.4611229
[107] loss: 244.9863631
[108] loss: 243.4766878
[109] loss: 239.6940000
[110] loss: 240.1814320
[111] loss: 238.4801127
[112] loss: 239.6306660
[113] loss: 239.3456957
[114] loss: 236.6025968
[115] loss: 233.5711701
[116] loss: 232.9574380
[117] loss: 232.0748576
[118] loss: 229.1758201
[119] loss: 228.5132667
[120] loss: 226.0095557
[121] loss: 225.8017410
[122] loss: 227.8134376
[123] loss: 223.4669644
[124] loss: 225.9021987
[125] loss: 225.0783857
[126] loss: 222.1138802
[127] loss: 219.7358098
[128] loss: 218.6550005
[129] loss: 216.6067517
[130] loss: 216.4056063
[131] loss: 215.0037590
[132] loss: 211.2700992
[133] loss: 214.9618598
[134] loss: 211.7828085
[135] loss: 211.3838979
[136] loss: 207.5170136
[137] loss: 209.9076697
[138] loss: 209.6365383
[139] loss: 204.9365283
[140] loss: 204.5779059
[141] loss: 205.6925734
[142] loss: 204.1568117
[143] loss: 202.2040712
[144] loss: 200.4171857
[145] loss: 199.7549943
[146] loss: 199.2271023
[147] loss: 199.1190881
[148] loss: 197.4724068
[149] loss: 197.2694456
[150] loss: 194.0623146
[151] loss: 197.5293941
[152] loss: 191.8829384
[153] loss: 193.3614893
[154] loss: 196.0001923
[155] loss: 190.6016517
[156] loss: 187.1026807
[157] loss: 188.8425248
[158] loss: 191.4013834
[159] loss: 188.3324546
[160] loss: 186.4942226
[161] loss: 185.7895977
[162] loss: 189.5468880
[163] loss: 183.4114728
[164] loss: 184.5891279
[165] loss: 181.3360762
[166] loss: 184.6242254
[167] loss: 180.0207873
[168] loss: 183.3724150
[169] loss: 177.6638176
[170] loss: 182.3237592
[171] loss: 178.2714339
[172] loss: 177.6719169
[173] loss: 177.0613882
[174] loss: 176.0165876
[175] loss: 175.7034968
[176] loss: 169.9611878
[177] loss: 172.7446980
[178] loss: 170.9135352
[179] loss: 171.9641181
[180] loss: 172.9563288
[181] loss: 171.2232637
[182] loss: 167.3485494
[183] loss: 166.6585441
[184] loss: 169.8911442
[185] loss: 166.5768121
[186] loss: 164.0164615
[187] loss: 163.9173532
[188] loss: 164.4656563
[189] loss: 162.7911139
[190] loss: 163.3616476
[191] loss: 162.4091447
[192] loss: 163.0508016
[193] loss: 162.1358273
[194] loss: 158.7685391
[195] loss: 161.9292823
[196] loss: 159.6211170
[197] loss: 158.0095372
[198] loss: 157.0920534
[199] loss: 155.9583688
[200] loss: 155.1903014
[201] loss: 153.5896526
[202] loss: 156.6441978
[203] loss: 156.5589414
[204] loss: 154.3538520
[205] loss: 150.8504414
[206] loss: 154.8736384
[207] loss: 150.6341612
[208] loss: 152.0820922
[209] loss: 149.6602831
[210] loss: 148.6477976
[211] loss: 149.2322468
[212] loss: 146.7521570
[213] loss: 144.8956392
[214] loss: 148.5860735
[215] loss: 149.3419460
[216] loss: 145.4262871
[217] loss: 147.0077325
[218] loss: 148.1683252
[219] loss: 145.3648267
[220] loss: 145.5535120
[221] loss: 144.7592306
[222] loss: 140.0154188
[223] loss: 139.5468946
[224] loss: 143.7486151
[225] loss: 138.0433766
[226] loss: 142.6221701
[227] loss: 140.9652587
[228] loss: 139.0845645
[229] loss: 140.0832341
[230] loss: 141.8487577
[231] loss: 137.6290640
[232] loss: 137.3392526
[233] loss: 134.5015496
[234] loss: 135.2103453
[235] loss: 134.6943749
[236] loss: 135.6931104
[237] loss: 137.8098788
[238] loss: 133.2885439
[239] loss: 134.7097490
[240] loss: 130.9456156
[241] loss: 136.4841000
[242] loss: 132.5892600
[243] loss: 132.9585654
[244] loss: 133.8567386
[245] loss: 131.0359003
[246] loss: 130.5621481
[247] loss: 132.0376492
[248] loss: 130.1891619
[249] loss: 127.5760866
[250] loss: 131.8360155
[251] loss: 126.0629991
[252] loss: 129.0234960
[253] loss: 128.7866207
[254] loss: 128.9757146
[255] loss: 127.7245874
[256] loss: 127.9055765
[257] loss: 124.7361412
[258] loss: 125.2521025
[259] loss: 123.8800051
[260] loss: 123.5133739
[261] loss: 121.9031761
[262] loss: 124.4468831
[263] loss: 123.2748742
[264] loss: 123.3499646
[265] loss: 121.8542814
[266] loss: 125.3579918
[267] loss: 119.7243774
[268] loss: 120.4547282
[269] loss: 118.1874886
[270] loss: 122.3840784
[271] loss: 119.8495118
[272] loss: 118.5769319
[273] loss: 119.6918971
[274] loss: 118.8441158
[275] loss: 116.6996597
[276] loss: 119.0761357
[277] loss: 120.8496912
[278] loss: 118.5988131
[279] loss: 118.3460666
[280] loss: 118.1348018
[281] loss: 115.6907573
Finished Training
The r2m value for this run is:  0.655
The AUPR for this run is:  0.681
The Concordance Index (CI) for this run is:  0.875
The Mean Squared Error (MSE) for this run is:  0.259
r2m std is:  0.007
AUPR std is:  0.019
CIs std is:  0.005
Best r2m was:  0.6363491128121634
Best parameters were: [20, 0.001, 281]


PS:

Read file <stderr_davis_unshuff.log> for stderr output of this job.

