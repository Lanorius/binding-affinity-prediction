  0%|          | 0/5 [00:00<?, ?it/s]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [3:56:22<00:00, 14182.15s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [3:56:22<00:00, 14182.15s/it]
 20%|â–ˆâ–ˆ        | 1/5 [3:56:22<15:45:28, 14182.15s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [3:55:21<00:00, 14121.77s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [3:55:21<00:00, 14121.77s/it]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [7:51:43<11:47:19, 14146.63s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [3:49:58<00:00, 13798.91s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [3:49:58<00:00, 13798.91s/it]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [11:41:42<7:46:15, 13987.86s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [4:01:06<00:00, 14466.95s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [4:01:06<00:00, 14466.96s/it]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [15:42:49<3:56:17, 14177.00s/it]
  0%|          | 0/1 [00:00<?, ?it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [3:54:06<00:00, 14046.29s/it][A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [3:54:06<00:00, 14046.29s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [19:36:56<00:00, 14129.87s/it]  100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [19:36:56<00:00, 14123.22s/it]
Traceback (most recent call last):
  File "/mnt/project/protdrugaffinity/dg/binding-affinity-prediction/src/binding_prediction.py", line 193, in <module>
    training_loss_per_epoch = model_manager.train(train_loader, best_parameters_overall[2], best_parameters_overall[0],
  File "/mnt/project/protdrugaffinity/dg/binding-affinity-prediction/src/models/training.py", line 140, in train
    loss_per_epoch = self.trainer.train(self.model, data_for_training, amount_of_epochs, batch_size, final_training)
  File "/mnt/project/protdrugaffinity/dg/binding-affinity-prediction/src/models/training.py", line 56, in train
    self.optimizer.step()
  File "/mnt/lsf-nas-1/os-shared/anaconda3/envs/daniel/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 26, in decorate_context
    return func(*args, **kwargs)
  File "/mnt/lsf-nas-1/os-shared/anaconda3/envs/daniel/lib/python3.9/site-packages/torch/optim/adam.py", line 108, in step
    F.adam(params_with_grad,
  File "/mnt/lsf-nas-1/os-shared/anaconda3/envs/daniel/lib/python3.9/site-packages/torch/optim/functional.py", line 94, in adam
    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(eps)
KeyboardInterrupt
