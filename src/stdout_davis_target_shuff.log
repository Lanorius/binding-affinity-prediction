Sender: LSF System <lsfadmin@lsf-server-3.rostclust>
Subject: Job 13518: <dg_prediction_davis_target_shuff> in cluster <rost_lsf_cluster_1> Done

Job <dg_prediction_davis_target_shuff> was submitted from host <lsf-master-1.rostclust> by user <giessing> in cluster <rost_lsf_cluster_1> at Tue Jan  4 14:37:04 2022
Job was executed on host(s) <lsf-server-3.rostclust>, in queue <low-end-normal>, as user <giessing> in cluster <rost_lsf_cluster_1> at Tue Jan  4 14:37:05 2022
</mnt/home/giessing> was used as the home directory.
</mnt/project/protdrugaffinity/dg/binding-affinity-prediction/src> was used as the working directory.
Started at Tue Jan  4 14:37:05 2022
Terminated at Sat Jan  8 04:02:35 2022
Results reported at Sat Jan  8 04:02:35 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python binding_prediction.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1023276.50 sec.
    Max Memory :                                 1677 MB
    Average Memory :                             1672.06 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                26
    Run time :                                   307529 sec.
    Turnaround time :                            307531 sec.

The output (if any) follows:

(['pkd', 'Davis'], 'chemVAE', <Section: INPUT FILES>, True, 1, False, True)
Using device: cuda
[55, 0.0001, 172]
[25, 0.001, 261]
[25, 0.001, 261]
[155, 0.001, 170]
[220, 0.001, 217]
[220, 0.0001, 264]
[220, 0.0001, 264]
[220, 0.0001, 264]
[220, 0.0001, 264]
[220, 0.0001, 264]
[220, 0.0001, 264]
[220, 0.0001, 264]
[220, 0.0001, 264]
[60, 0.0001, 272]
[60, 0.0001, 272]
[60, 0.0001, 272]
[60, 0.0001, 272]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
[235, 0.01, 196]
Finished Tuning
0.4025956578815188
[235, 0.01, 196]
[1] loss: 8152.3049054
[2] loss: 2861.3768862
[3] loss: 2861.3768862
[4] loss: 2861.3768862
[5] loss: 2861.3768862
[6] loss: 2861.3768862
[7] loss: 2861.3768862
[8] loss: 2861.3768862
[9] loss: 2861.3768862
[10] loss: 2861.3768862
[11] loss: 2861.3768862
[12] loss: 4976.2194668
[13] loss: 2861.3768862
[14] loss: 2861.3768862
[15] loss: 2861.3768862
[16] loss: 2861.3768862
[17] loss: 2861.3768862
[18] loss: 2861.3768862
[19] loss: 2861.3768862
[20] loss: 2861.3768862
[21] loss: 2861.3768862
[22] loss: 2861.3768862
[23] loss: 2861.3768862
[24] loss: 2861.3768862
[25] loss: 2861.3768862
[26] loss: 2861.3768862
[27] loss: 2861.3768862
[28] loss: 2861.3768862
[29] loss: 2861.3768862
[30] loss: 2861.3768862
[31] loss: 2861.3768862
[32] loss: 2861.3768862
[33] loss: 2861.3768862
[34] loss: 2861.3768862
[35] loss: 2861.3768862
[36] loss: 2861.3768862
[37] loss: 2861.3768862
[38] loss: 2861.3768862
[39] loss: 2861.3768862
[40] loss: 2861.3768862
[41] loss: 2861.3768862
[42] loss: 2861.3768862
[43] loss: 2861.3768862
[44] loss: 2861.3768862
[45] loss: 2861.3768862
[46] loss: 2861.3768862
[47] loss: 2861.3768862
[48] loss: 2861.3768862
[49] loss: 2861.3768862
[50] loss: 2861.3768862
[51] loss: 2861.3768862
[52] loss: 2861.3768862
[53] loss: 2861.3768862
[54] loss: 2861.3768862
[55] loss: 2861.3768862
[56] loss: 2861.3768862
[57] loss: 2861.3768862
[58] loss: 2861.3768862
[59] loss: 2861.3768862
[60] loss: 2861.3768862
[61] loss: 2861.3768862
[62] loss: 2861.3768862
[63] loss: 2861.3768862
[64] loss: 2861.3768862
[65] loss: 2861.3768862
[66] loss: 2861.3768862
[67] loss: 2861.3768862
[68] loss: 2861.3768862
[69] loss: 2861.3768862
[70] loss: 2861.3768862
[71] loss: 2861.3768862
[72] loss: 2861.3768862
[73] loss: 2861.3768862
[74] loss: 2861.3768862
[75] loss: 2861.3768862
[76] loss: 2861.3768862
[77] loss: 2861.3768862
[78] loss: 2861.3768862
[79] loss: 2861.3768862
[80] loss: 2861.3768862
[81] loss: 2861.3768862
[82] loss: 2861.3768862
[83] loss: 2861.3768862
[84] loss: 2861.3768862
[85] loss: 2861.3768862
[86] loss: 2861.3768862
[87] loss: 2861.3768862
[88] loss: 2861.3768862
[89] loss: 2861.3768862
[90] loss: 2861.3768862
[91] loss: 2861.3768862
[92] loss: 2861.3768862
[93] loss: 2861.3768862
[94] loss: 2861.3768862
[95] loss: 2861.3768862
[96] loss: 2861.3768862
[97] loss: 2861.3768862
[98] loss: 2861.3768862
[99] loss: 2861.3768862
[100] loss: 2861.3768862
[101] loss: 2861.3768862
[102] loss: 2861.3768862
[103] loss: 2861.3768862
[104] loss: 2861.3768862
[105] loss: 2861.3768862
[106] loss: 2861.3768862
[107] loss: 2861.3768862
[108] loss: 2861.3768862
[109] loss: 2861.3768862
[110] loss: 2861.3768862
[111] loss: 2861.3768862
[112] loss: 2861.3768862
[113] loss: 2861.3768862
[114] loss: 2861.3768862
[115] loss: 2861.3768862
[116] loss: 2861.3768862
[117] loss: 2861.3768862
[118] loss: 2861.3768862
[119] loss: 2861.3768862
[120] loss: 2861.3768862
[121] loss: 2861.3768862
[122] loss: 2861.3768862
[123] loss: 2861.3768862
[124] loss: 2861.3768862
[125] loss: 2861.3768862
[126] loss: 2861.3768862
[127] loss: 2861.3768862
[128] loss: 2861.3768862
[129] loss: 2861.3768862
[130] loss: 2861.3768862
[131] loss: 2861.3768862
[132] loss: 2861.3768862
[133] loss: 2861.3768862
[134] loss: 2861.3768862
[135] loss: 2861.3768862
[136] loss: 2861.3768862
[137] loss: 2861.3768862
[138] loss: 2861.3768862
[139] loss: 2861.3768862
[140] loss: 2861.3768862
[141] loss: 2861.3768862
[142] loss: 2861.3768862
[143] loss: 2861.3768862
[144] loss: 2861.3768862
[145] loss: 2861.3768862
[146] loss: 2861.3768862
[147] loss: 2861.3768862
[148] loss: 2861.3768862
[149] loss: 2861.3768862
[150] loss: 2861.3768862
[151] loss: 2861.3768862
[152] loss: 2861.3768862
[153] loss: 2861.3768862
[154] loss: 2861.3768862
[155] loss: 2861.3768862
[156] loss: 2861.3768862
[157] loss: 2861.3768862
[158] loss: 2861.3768862
[159] loss: 2861.3768862
[160] loss: 2861.3768862
[161] loss: 2861.3768862
[162] loss: 2861.3768862
[163] loss: 2861.3768862
[164] loss: 2861.3768862
[165] loss: 2861.3768862
[166] loss: 2861.3768862
[167] loss: 2861.3768862
[168] loss: 2861.3768862
[169] loss: 2861.3768862
[170] loss: 2861.3768862
[171] loss: 2861.3768862
[172] loss: 2861.3768862
[173] loss: 2861.3768862
[174] loss: 2861.3768862
[175] loss: 2861.3768862
[176] loss: 2861.3768862
[177] loss: 2861.3768862
[178] loss: 2861.3768862
[179] loss: 2861.3768862
[180] loss: 2861.3768862
[181] loss: 2861.3768862
[182] loss: 2861.3768862
[183] loss: 2861.3768862
[184] loss: 2861.3768862
[185] loss: 2861.3768862
[186] loss: 2861.3768862
[187] loss: 2861.3768862
[188] loss: 2861.3768862
[189] loss: 2861.3768862
[190] loss: 2861.3768862
[191] loss: 2861.3768862
[192] loss: 2861.3768862
[193] loss: 2861.3768862
[194] loss: 2861.3768862
[195] loss: 2861.3768862
[196] loss: 2861.3768862
Finished Training
The r2m value for this run is:  nan
The AUPR for this run is:  0.542
The Concordance Index (CI) for this run is:  0.5
The Mean Squared Error (MSE) for this run is:  30.561
r2m std is:  nan
AUPR std is:  0.0
CIs std is:  0.0
Best parameters were: [235, 0.01, 196]


PS:

Read file <stderr_davis_target_shuff.log> for stderr output of this job.

