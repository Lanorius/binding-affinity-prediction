Sender: LSF System <lsfadmin@lsf-server-3.rostclust>
Subject: Job 13655: <dg_prediction_davis_drug_shuff_short> in cluster <rost_lsf_cluster_1> Exited

Job <dg_prediction_davis_drug_shuff_short> was submitted from host <lsf-master-1.rostclust> by user <giessing> in cluster <rost_lsf_cluster_1> at Tue Jan 11 09:10:21 2022
Job was executed on host(s) <lsf-server-3.rostclust>, in queue <low-end-normal>, as user <giessing> in cluster <rost_lsf_cluster_1> at Tue Jan 11 09:10:21 2022
</mnt/home/giessing> was used as the home directory.
</mnt/project/protdrugaffinity/dg/binding-affinity-prediction/src> was used as the working directory.
Started at Tue Jan 11 09:10:21 2022
Terminated at Wed Jan 12 08:44:35 2022
Results reported at Wed Jan 12 08:44:35 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python binding_prediction.py
------------------------------------------------------------

TERM_OWNER: job killed by owner.
Exited with exit code 130.

Resource usage summary:

    CPU time :                                   300293.47 sec.
    Max Memory :                                 1657 MB
    Average Memory :                             1652.05 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                26
    Run time :                                   84853 sec.
    Turnaround time :                            84854 sec.

The output (if any) follows:

(['pkd', 'Davis'], 'chemVAE', <Section: INPUT FILES>, True, 1, True, False)
Using device: cuda
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
Finished Tuning
0.618115837387019
[30, 0.001, 274]
Training attempt No: 1
[1] loss: 712.2436538
[2] loss: 544.9775239
[3] loss: 491.0009714
[4] loss: 458.6104247
[5] loss: 435.8444127
[6] loss: 418.0567138
[7] loss: 397.2328524
[8] loss: 384.8189931
[9] loss: 378.1862738
[10] loss: 372.4017041
[11] loss: 361.1841326
[12] loss: 354.4160391
[13] loss: 350.4267213
[14] loss: 342.1348025
[15] loss: 337.1931193
[16] loss: 331.1832470
[17] loss: 326.8172470
[18] loss: 321.3011825
[19] loss: 315.0353880
[20] loss: 314.2501580
[21] loss: 315.0644005
[22] loss: 307.3700911
[23] loss: 307.0421937
[24] loss: 298.5747671
[25] loss: 299.9381899
[26] loss: 297.0009012
[27] loss: 291.9714835
[28] loss: 290.0942194
[29] loss: 285.8810692
[30] loss: 288.0971860
[31] loss: 283.0088223
[32] loss: 280.6736382
[33] loss: 278.1957893
[34] loss: 277.9121614
[35] loss: 276.0914372
[36] loss: 271.0502554
[37] loss: 268.7244583
[38] loss: 265.2139828
[39] loss: 266.2829337
[40] loss: 265.7650198
[41] loss: 262.6366970
[42] loss: 260.4695239
[43] loss: 261.0420813
[44] loss: 256.1697241
[45] loss: 254.9531369
[46] loss: 253.9599037
[47] loss: 250.5331014
[48] loss: 250.9119684
[49] loss: 247.3989491
[50] loss: 246.8473340
[51] loss: 245.8251925
[52] loss: 243.7366786
[53] loss: 243.8294741
[54] loss: 240.5820437
[55] loss: 239.3955024
[56] loss: 235.9409464
[57] loss: 237.2797404
[58] loss: 236.2631471
[59] loss: 234.0622443
[60] loss: 233.0562285
[61] loss: 233.8289717
[62] loss: 229.8014306
[63] loss: 230.6382522
[64] loss: 229.9856685
[65] loss: 227.3805997
[66] loss: 227.1881032
[67] loss: 225.2037404
[68] loss: 224.0851954
[69] loss: 222.2203483
[70] loss: 221.5642885
[71] loss: 220.7457754
[72] loss: 221.0653754
[73] loss: 221.9965673
[74] loss: 218.0916488
[75] loss: 216.5554278
[76] loss: 215.2161693
[77] loss: 212.6256355
[78] loss: 213.0262151
[79] loss: 212.1907313
[80] loss: 211.9939940
[81] loss: 211.2876469
[82] loss: 208.0080858
[83] loss: 210.2815537
[84] loss: 209.4650337
[85] loss: 206.4888719
[86] loss: 206.4089634
[87] loss: 206.1405018
[88] loss: 204.0417114
[89] loss: 205.5171736
[90] loss: 206.8604806
[91] loss: 203.0613155
[92] loss: 202.9121544
[93] loss: 201.0944038
[94] loss: 200.7388597
[95] loss: 198.9735765
[96] loss: 197.4799351
[97] loss: 196.9047373
[98] loss: 196.4046529
[99] loss: 194.5678009
[100] loss: 193.2794652
[101] loss: 193.3047948
[102] loss: 193.4710787
[103] loss: 194.4796949
[104] loss: 193.0535515
[105] loss: 190.3869885
[106] loss: 189.6083981
[107] loss: 189.0275407
[108] loss: 186.5681018
[109] loss: 186.6733869
[110] loss: 184.8032526
[111] loss: 184.9555817
[112] loss: 184.2685131
[113] loss: 183.5463345
[114] loss: 182.3031168
[115] loss: 182.6100508
[116] loss: 181.9169148
[117] loss: 179.6942960
[118] loss: 179.7813728
[119] loss: 178.5619473
[120] loss: 178.2440227
[121] loss: 178.1073973
[122] loss: 176.3992016
[123] loss: 174.4171017
[124] loss: 173.6298305
[125] loss: 173.2319783
[126] loss: 171.7398971
[127] loss: 172.2723957
[128] loss: 172.2611167
[129] loss: 169.9298284
[130] loss: 170.2077021
[131] loss: 166.7376394
[132] loss: 167.5609653
[133] loss: 166.9965777
[134] loss: 165.5415476
[135] loss: 164.7614949
[136] loss: 161.8365597
[137] loss: 161.5186725
[138] loss: 162.1128064
[139] loss: 161.2008245
[140] loss: 160.5051441
[141] loss: 159.3674578
[142] loss: 159.1976783
[143] loss: 158.0734263
[144] loss: 157.9248380
[145] loss: 157.1585536
[146] loss: 156.5210334
[147] loss: 155.0845889
[148] loss: 154.8231157
[149] loss: 155.5169270
[150] loss: 153.2779123
[151] loss: 153.3955402
[152] loss: 153.7893411
[153] loss: 152.5023103
[154] loss: 149.2176590
[155] loss: 149.9560434
[156] loss: 150.4919048
[157] loss: 148.2331936
[158] loss: 147.2469969
[159] loss: 149.1319053
[160] loss: 145.7210007
[161] loss: 145.8894866
[162] loss: 144.5731935
[163] loss: 144.5037433
[164] loss: 145.6081070
[165] loss: 143.6861719
[166] loss: 143.4315588
[167] loss: 143.1392033
[168] loss: 142.3739276
[169] loss: 141.9622728
[170] loss: 139.6850196
[171] loss: 139.6017177
[172] loss: 139.3198459
[173] loss: 139.7467289
[174] loss: 137.6492438
[175] loss: 138.2465099
[176] loss: 136.6566555
[177] loss: 136.6808391
[178] loss: 135.8663164
[179] loss: 134.0066631
[180] loss: 133.8707745
[181] loss: 135.4084115
[182] loss: 134.5468035
[183] loss: 131.5395193
[184] loss: 132.8551478
[185] loss: 132.1449208
[186] loss: 133.8716537
[187] loss: 130.2646558
[188] loss: 131.5742942
[189] loss: 128.5440574
[190] loss: 128.8217180
[191] loss: 128.2295609
[192] loss: 127.8753123
[193] loss: 126.1940514
[194] loss: 125.7257296
[195] loss: 124.2969835
[196] loss: 125.7996420
[197] loss: 125.6807817
[198] loss: 123.5468412
[199] loss: 122.7286984
[200] loss: 123.1256288
[201] loss: 120.3865060
[202] loss: 122.6662281
[203] loss: 121.0556310
[204] loss: 120.7899661
[205] loss: 121.2462716
[206] loss: 119.8113491
[207] loss: 117.4088565
[208] loss: 119.0101949
[209] loss: 118.5697193
[210] loss: 118.4671575
[211] loss: 116.2515797
[212] loss: 115.3281333
[213] loss: 116.2165124
[214] loss: 117.8842528
[215] loss: 116.2451277
[216] loss: 115.9747869
[217] loss: 115.1923374
[218] loss: 114.3925028


PS:

Read file <stderr_davis_drug_shuff_short.log> for stderr output of this job.

