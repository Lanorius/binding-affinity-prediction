Sender: LSF System <lsfadmin@lsf-server-3.rostclust>
Subject: Job 13746: <dg_prediction_davis> in cluster <rost_lsf_cluster_1> Done

Job <dg_prediction_davis> was submitted from host <lsf-master-1.rostclust> by user <giessing> in cluster <rost_lsf_cluster_1> at Fri Jan 14 13:17:40 2022
Job was executed on host(s) <lsf-server-3.rostclust>, in queue <low-end-normal>, as user <giessing> in cluster <rost_lsf_cluster_1> at Fri Jan 14 13:17:42 2022
</mnt/home/giessing> was used as the home directory.
</mnt/project/protdrugaffinity/dg/binding-affinity-prediction/src> was used as the working directory.
Started at Fri Jan 14 13:17:42 2022
Terminated at Sat Jan 15 10:20:45 2022
Results reported at Sat Jan 15 10:20:45 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python binding_prediction.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   186690.83 sec.
    Max Memory :                                 1680 MB
    Average Memory :                             1672.44 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                26
    Run time :                                   75782 sec.
    Turnaround time :                            75785 sec.

The output (if any) follows:

Using device: cuda
[143, 0.01, 265]
[143, 0.01, 265]
[228, 0.001, 243]
[228, 0.001, 243]
[143, 0.001, 213]
[143, 0.001, 213]
[143, 0.001, 213]
[143, 0.001, 213]
[143, 0.001, 213]
[143, 0.001, 213]
[143, 0.001, 213]
[143, 0.001, 213]
[153, 0.0001, 275]
[153, 0.0001, 275]
[153, 0.0001, 275]
[153, 0.0001, 275]
[193, 0.0001, 288]
[193, 0.0001, 288]
[193, 0.0001, 288]
[193, 0.0001, 288]
[193, 0.0001, 288]
[133, 0.0001, 270]
[133, 0.0001, 270]
[133, 0.0001, 270]
[193, 0.0001, 282]
Finished Tuning
0.5763556921662615
[193, 0.0001, 282]
Training attempt No: 1
[1] loss: 449.3950913
[2] loss: 70.9601402
[3] loss: 69.4390287
[4] loss: 66.8586767
[5] loss: 65.3208474
[6] loss: 64.7749532
[7] loss: 63.7858079
[8] loss: 63.4954274
[9] loss: 62.7559737
[10] loss: 62.0327245
[11] loss: 61.5648416
[12] loss: 60.6886012
[13] loss: 59.7883113
[14] loss: 59.1899113
[15] loss: 58.2889998
[16] loss: 57.2906485
[17] loss: 57.0532595
[18] loss: 56.6860321
[19] loss: 56.5249821
[20] loss: 55.6330960
[21] loss: 55.1047508
[22] loss: 54.8704295
[23] loss: 54.9420058
[24] loss: 54.0958425
[25] loss: 54.5136378
[26] loss: 53.9415406
[27] loss: 53.4718746
[28] loss: 53.1523274
[29] loss: 52.6618717
[30] loss: 52.6507444
[31] loss: 52.4108914
[32] loss: 51.5599262
[33] loss: 51.1223442
[34] loss: 50.9024313
[35] loss: 50.5771744
[36] loss: 50.2633995
[37] loss: 49.8980027
[38] loss: 49.8384025
[39] loss: 49.7207247
[40] loss: 48.8725996
[41] loss: 48.5799225
[42] loss: 47.9628209
[43] loss: 47.4627894
[44] loss: 47.6574973
[45] loss: 47.2209100
[46] loss: 47.1131887
[47] loss: 46.1585137
[48] loss: 45.6851304
[49] loss: 45.6721350
[50] loss: 45.3653138
[51] loss: 44.9631184
[52] loss: 44.5005244
[53] loss: 44.7095899
[54] loss: 44.0842474
[55] loss: 43.8763121
[56] loss: 43.5728941
[57] loss: 43.2821555
[58] loss: 43.2469785
[59] loss: 42.7606152
[60] loss: 42.5853174
[61] loss: 42.5632227
[62] loss: 42.4187912
[63] loss: 41.9889673
[64] loss: 41.6351719
[65] loss: 41.5240926
[66] loss: 41.4043363
[67] loss: 41.1061287
[68] loss: 41.1954680
[69] loss: 40.8680605
[70] loss: 40.4794132
[71] loss: 40.4499488
[72] loss: 40.2416751
[73] loss: 40.1362348
[74] loss: 39.7135863
[75] loss: 39.8641278
[76] loss: 39.4203495
[77] loss: 39.2317443
[78] loss: 39.3183830
[79] loss: 38.7672513
[80] loss: 38.4167381
[81] loss: 38.3672193
[82] loss: 38.2968061
[83] loss: 37.7790756
[84] loss: 37.7219536
[85] loss: 37.9008269
[86] loss: 37.3573541
[87] loss: 37.0503128
[88] loss: 37.6807158
[89] loss: 36.6930060
[90] loss: 37.1715250
[91] loss: 36.8216393
[92] loss: 36.6625811
[93] loss: 36.2155973
[94] loss: 36.4108070
[95] loss: 36.2811397
[96] loss: 36.1702655
[97] loss: 35.9911632
[98] loss: 36.1909609
[99] loss: 36.2659995
[100] loss: 35.6984167
[101] loss: 35.7507040
[102] loss: 35.8236025
[103] loss: 35.3740165
[104] loss: 35.5284185
[105] loss: 35.2345587
[106] loss: 35.1067515
[107] loss: 35.1300525
[108] loss: 35.1889349
[109] loss: 34.9961903
[110] loss: 34.8014912
[111] loss: 35.0595967
[112] loss: 34.5159852
[113] loss: 34.5586754
[114] loss: 34.5573669
[115] loss: 34.2613593
[116] loss: 34.3734063
[117] loss: 33.8044355
[118] loss: 33.9531136
[119] loss: 33.8015023
[120] loss: 33.7496480
[121] loss: 33.5355043
[122] loss: 33.8051244
[123] loss: 33.7512230
[124] loss: 33.7123187
[125] loss: 33.3814998
[126] loss: 33.3441394
[127] loss: 33.1656785
[128] loss: 33.0796963
[129] loss: 33.2823302
[130] loss: 33.3401861
[131] loss: 33.0958795
[132] loss: 32.8776252
[133] loss: 32.4867568
[134] loss: 33.0504919
[135] loss: 33.0082391
[136] loss: 32.3069113
[137] loss: 32.4082157
[138] loss: 32.1046237
[139] loss: 32.3083260
[140] loss: 32.3375329
[141] loss: 32.0649744
[142] loss: 32.1535701
[143] loss: 31.7790386
[144] loss: 31.9531142
[145] loss: 31.9465598
[146] loss: 31.7599601
[147] loss: 31.7221771
[148] loss: 31.6283002
[149] loss: 31.4264952
[150] loss: 31.5026572
[151] loss: 31.4169708
[152] loss: 31.1030231
[153] loss: 31.3383773
[154] loss: 31.0680890
[155] loss: 31.0465057
[156] loss: 31.0840002
[157] loss: 31.1275538
[158] loss: 30.7057564
[159] loss: 31.1564773
[160] loss: 30.9731746
[161] loss: 30.8366078
[162] loss: 30.7609818
[163] loss: 30.4702602
[164] loss: 30.3020652
[165] loss: 30.5855520
[166] loss: 30.0484687
[167] loss: 30.4333758
[168] loss: 29.8725977
[169] loss: 29.8760073
[170] loss: 29.9197745
[171] loss: 29.7906552
[172] loss: 29.8488589
[173] loss: 29.5491175
[174] loss: 29.6099315
[175] loss: 29.7832010
[176] loss: 29.7105641
[177] loss: 29.4807127
[178] loss: 29.3258118
[179] loss: 29.2477858
[180] loss: 29.3776059
[181] loss: 29.3908635
[182] loss: 29.1884985
[183] loss: 29.0579474
[184] loss: 28.8309008
[185] loss: 28.7112357
[186] loss: 28.9045720
[187] loss: 28.8454307
[188] loss: 28.6018268
[189] loss: 28.5598457
[190] loss: 28.5168564
[191] loss: 28.3238679
[192] loss: 28.2722366
[193] loss: 28.3332836
[194] loss: 28.4319221
[195] loss: 28.0408081
[196] loss: 28.0682011
[197] loss: 28.2359009
[198] loss: 27.8738348
[199] loss: 27.6403481
[200] loss: 27.9193793
[201] loss: 27.7740220
[202] loss: 27.4691661
[203] loss: 27.2110594
[204] loss: 27.6226582
[205] loss: 27.2976112
[206] loss: 27.2102813
[207] loss: 27.6122124
[208] loss: 26.9775828
[209] loss: 27.0569157
[210] loss: 26.8864075
[211] loss: 27.2876915
[212] loss: 26.6294260
[213] loss: 26.6774320
[214] loss: 26.6875824
[215] loss: 26.5287831
[216] loss: 26.4452648
[217] loss: 26.4362946
[218] loss: 26.3672832
[219] loss: 26.1675048
[220] loss: 26.3453126
[221] loss: 26.2297494
[222] loss: 26.4006608
[223] loss: 26.0924644
[224] loss: 25.7229533
[225] loss: 25.8945552
[226] loss: 25.7743777
[227] loss: 25.9320662
[228] loss: 25.4764255
[229] loss: 25.6495117
[230] loss: 25.1510233
[231] loss: 25.7304776
[232] loss: 25.1834089
[233] loss: 25.6291911
[234] loss: 25.1444113
[235] loss: 24.9403123
[236] loss: 25.3691084
[237] loss: 24.9391786
[238] loss: 25.2842550
[239] loss: 24.9362384
[240] loss: 24.8447215
[241] loss: 24.8267507
[242] loss: 24.7626641
[243] loss: 24.8893576
[244] loss: 24.6918536
[245] loss: 24.7949577
[246] loss: 24.4071904
[247] loss: 24.3045246
[248] loss: 24.1554111
[249] loss: 24.3131026
[250] loss: 24.2389608
[251] loss: 24.0384894
[252] loss: 24.2796843
[253] loss: 23.8864499
[254] loss: 23.7427246
[255] loss: 23.6980580
[256] loss: 24.0426054
[257] loss: 23.6399880
[258] loss: 23.5396217
[259] loss: 23.6891346
[260] loss: 23.5188720
[261] loss: 23.3280347
[262] loss: 23.6727906
[263] loss: 23.1869914
[264] loss: 23.3676002
[265] loss: 23.2429566
[266] loss: 23.1386801
[267] loss: 22.8385433
[268] loss: 23.0333420
[269] loss: 23.0184654
[270] loss: 23.0197956
[271] loss: 22.7161704
[272] loss: 22.7483290
[273] loss: 22.8095134
[274] loss: 22.6498075
[275] loss: 22.4313894
[276] loss: 22.4326139
[277] loss: 22.2705940
[278] loss: 22.6151753
[279] loss: 22.2818868
[280] loss: 22.0417047
[281] loss: 22.1361853
[282] loss: 22.0566857
Finished Training
The r2m value for this run is:  0.636
The AUPR for this run is:  0.691
The Concordance Index (CI) for this run is:  0.872
The Mean Squared Error (MSE) for this run is:  0.26
r2m std is:  0.011
AUPR std is:  0.016
CIs std is:  0.004
Best r2m was:  0.5763556921662615
Best parameters were: [193, 0.0001, 282]


PS:

Read file <stderr_davis.log> for stderr output of this job.

