Sender: LSF System <lsfadmin@lsf-server-3.rostclust>
Subject: Job 14110: <dg_prediction_davis> in cluster <rost_lsf_cluster_1> Done

Job <dg_prediction_davis> was submitted from host <lsf-master-1.rostclust> by user <giessing> in cluster <rost_lsf_cluster_1> at Mon Jan 24 15:41:41 2022
Job was executed on host(s) <lsf-server-3.rostclust>, in queue <low-end-normal>, as user <giessing> in cluster <rost_lsf_cluster_1> at Mon Jan 24 15:41:41 2022
</mnt/home/giessing> was used as the home directory.
</mnt/project/protdrugaffinity/dg/binding-affinity-prediction/src> was used as the working directory.
Started at Mon Jan 24 15:41:41 2022
Terminated at Tue Jan 25 05:49:45 2022
Results reported at Tue Jan 25 05:49:45 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python binding_prediction.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   197650.81 sec.
    Max Memory :                                 1695 MB
    Average Memory :                             1681.75 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                26
    Run time :                                   50884 sec.
    Turnaround time :                            50884 sec.

The output (if any) follows:

Using device: cuda
Finished Tuning
[216, 0.0001, 367]
Training attempt No: 1
[1] loss: 13.1626316
[2] loss: 12.9769584
[3] loss: 12.5538537
[4] loss: 12.2993621
[5] loss: 12.1117460
[6] loss: 12.0429313
[7] loss: 11.8674391
[8] loss: 11.6916586
[9] loss: 11.6370036
[10] loss: 11.4718896
[11] loss: 11.4430576
[12] loss: 11.2342517
[13] loss: 11.0735750
[14] loss: 10.9805341
[15] loss: 10.8297472
[16] loss: 10.7920304
[17] loss: 10.6088606
[18] loss: 10.5592248
[19] loss: 10.4871699
[20] loss: 10.4440343
[21] loss: 10.4261814
[22] loss: 10.4163196
[23] loss: 10.4440368
[24] loss: 10.2865966
[25] loss: 10.1738178
[26] loss: 10.1065670
[27] loss: 9.9938267
[28] loss: 9.9897004
[29] loss: 10.1859449
[30] loss: 9.9413880
[31] loss: 9.6871933
[32] loss: 9.6776253
[33] loss: 9.7471443
[34] loss: 9.5817060
[35] loss: 9.4696025
[36] loss: 9.3945666
[37] loss: 9.1613397
[38] loss: 9.1390100
[39] loss: 9.0524327
[40] loss: 8.9589273
[41] loss: 8.7723661
[42] loss: 8.7547343
[43] loss: 8.7531140
[44] loss: 8.6981366
[45] loss: 8.3951116
[46] loss: 8.3518599
[47] loss: 8.5256125
[48] loss: 8.3177704
[49] loss: 8.3448420
[50] loss: 8.0067923
[51] loss: 8.0621872
[52] loss: 8.0980289
[53] loss: 7.9697864
[54] loss: 7.9269816
[55] loss: 7.9252485
[56] loss: 8.0235959
[57] loss: 7.8416934
[58] loss: 7.8586971
[59] loss: 7.8146871
[60] loss: 7.7386220
[61] loss: 7.5833632
[62] loss: 7.6284759
[63] loss: 7.7750579
[64] loss: 7.6769770
[65] loss: 7.6446016
[66] loss: 7.5702995
[67] loss: 7.4498021
[68] loss: 7.5585519
[69] loss: 7.5908142
[70] loss: 7.3962911
[71] loss: 7.3427415
[72] loss: 7.4990382
[73] loss: 7.3423219
[74] loss: 7.2385259
[75] loss: 7.2468731
[76] loss: 7.3843213
[77] loss: 7.1258868
[78] loss: 7.2500016
[79] loss: 7.1393634
[80] loss: 7.2149307
[81] loss: 7.0438807
[82] loss: 7.0337509
[83] loss: 7.0798464
[84] loss: 6.8703786
[85] loss: 7.0845893
[86] loss: 6.8452122
[87] loss: 6.9442185
[88] loss: 6.9857231
[89] loss: 6.9250929
[90] loss: 6.8296114
[91] loss: 6.7924737
[92] loss: 6.9022836
[93] loss: 6.8619184
[94] loss: 6.7595782
[95] loss: 6.9358965
[96] loss: 6.7491543
[97] loss: 6.7977399
[98] loss: 6.7258878
[99] loss: 6.8901074
[100] loss: 6.7222441
[101] loss: 6.7436772
[102] loss: 6.6687431
[103] loss: 6.6401811
[104] loss: 6.7397904
[105] loss: 6.7461193
[106] loss: 6.8298431
[107] loss: 6.4941560
[108] loss: 6.6261498
[109] loss: 6.6321756
[110] loss: 6.5002547
[111] loss: 6.7593374
[112] loss: 6.5187031
[113] loss: 6.6109908
[114] loss: 6.4684411
[115] loss: 6.4473386
[116] loss: 6.5461068
[117] loss: 6.5735507
[118] loss: 6.4917061
[119] loss: 6.4962794
[120] loss: 6.4843570
[121] loss: 6.5443295
[122] loss: 6.4121878
[123] loss: 6.5680096
[124] loss: 6.4763851
[125] loss: 6.4475949
[126] loss: 6.4632376
[127] loss: 6.4175396
[128] loss: 6.3026706
[129] loss: 6.3653428
[130] loss: 6.4999136
[131] loss: 6.5273536
[132] loss: 6.4132670
[133] loss: 6.3844374
[134] loss: 6.4315578
[135] loss: 6.3177551
[136] loss: 6.4509970
[137] loss: 6.3934516
[138] loss: 6.2268263
[139] loss: 6.3632377
[140] loss: 6.2950878
[141] loss: 6.3535134
[142] loss: 6.3718036
[143] loss: 6.4422657
[144] loss: 6.3707796
[145] loss: 6.2742460
[146] loss: 6.2309568
[147] loss: 6.3500016
[148] loss: 6.2040330
[149] loss: 6.3602590
[150] loss: 6.2916091
[151] loss: 6.3578871
[152] loss: 6.3975736
[153] loss: 6.2421790
[154] loss: 6.2412336
[155] loss: 6.2955449
[156] loss: 6.1045761
[157] loss: 6.2362230
[158] loss: 6.0748708
[159] loss: 6.0705777
[160] loss: 6.1902995
[161] loss: 6.1726248
[162] loss: 6.0638663
[163] loss: 6.2172699
[164] loss: 6.0368554
[165] loss: 6.0480506
[166] loss: 6.0270070
[167] loss: 5.9936257
[168] loss: 6.2384684
[169] loss: 5.9087395
[170] loss: 6.0690870
[171] loss: 6.1529312
[172] loss: 6.0022830
[173] loss: 6.0047535
[174] loss: 5.9955220
[175] loss: 6.0460845
[176] loss: 5.9444010
[177] loss: 6.1472946
[178] loss: 6.1514180
[179] loss: 5.9560957
[180] loss: 6.0651101
[181] loss: 6.2017720
[182] loss: 5.9886470
[183] loss: 6.0736948
[184] loss: 6.0800185
[185] loss: 6.1640680
[186] loss: 5.9762551
[187] loss: 5.8959519
[188] loss: 5.8071204
[189] loss: 6.0070388
[190] loss: 5.8577429
[191] loss: 5.9492758
[192] loss: 5.8211140
[193] loss: 5.9807208
[194] loss: 6.2529551
[195] loss: 5.8395529
[196] loss: 5.8037113
[197] loss: 5.8935912
[198] loss: 5.7751310
[199] loss: 5.8908698
[200] loss: 5.8864020
[201] loss: 5.6860960
[202] loss: 5.8444021
[203] loss: 5.7962031
[204] loss: 5.9136540
[205] loss: 5.7867330
[206] loss: 5.7904174
[207] loss: 5.7963595
[208] loss: 5.7809772
[209] loss: 5.9132902
[210] loss: 5.7865992
[211] loss: 5.8443324
[212] loss: 5.7656953
[213] loss: 5.8298800
[214] loss: 5.7404483
[215] loss: 5.6140861
[216] loss: 5.6802063
[217] loss: 5.6529069
[218] loss: 5.7028228
[219] loss: 5.6809848
[220] loss: 5.6742954
[221] loss: 5.5624781
[222] loss: 5.7054060
[223] loss: 5.7364341
[224] loss: 5.7849189
[225] loss: 5.7277739
[226] loss: 5.7465665
[227] loss: 5.5560386
[228] loss: 5.6347639
[229] loss: 5.6419842
[230] loss: 5.6185989
[231] loss: 5.7740693
[232] loss: 5.7043310
[233] loss: 5.7786904
[234] loss: 5.8332189
[235] loss: 5.5896370
[236] loss: 5.6018757
[237] loss: 5.7352252
[238] loss: 5.7302237
[239] loss: 5.6266951
[240] loss: 5.6181328
[241] loss: 5.6517680
[242] loss: 5.6218426
[243] loss: 5.6424383
[244] loss: 5.4507911
[245] loss: 5.5667438
[246] loss: 5.4736516
[247] loss: 5.5866920
[248] loss: 5.5104135
[249] loss: 5.4260232
[250] loss: 5.5138948
[251] loss: 5.5858120
[252] loss: 5.4256451
[253] loss: 5.4608829
[254] loss: 5.5764668
[255] loss: 5.5049094
[256] loss: 5.5713150
[257] loss: 5.3947743
[258] loss: 5.4811441
[259] loss: 5.5386785
[260] loss: 5.4855340
[261] loss: 5.5643991
[262] loss: 5.5649158
[263] loss: 5.5190268
[264] loss: 5.5553116
[265] loss: 5.4715152
[266] loss: 5.5433966
[267] loss: 5.4644760
[268] loss: 5.5445833
[269] loss: 5.3570529
[270] loss: 5.5138977
[271] loss: 5.3897502
[272] loss: 5.3035029
[273] loss: 5.5267627
[274] loss: 5.5546103
[275] loss: 5.4977855
[276] loss: 5.3813981
[277] loss: 5.3726962
[278] loss: 5.3311646
[279] loss: 5.4927893
[280] loss: 5.3404953
[281] loss: 5.3910112
[282] loss: 5.3815512
[283] loss: 5.3550114
[284] loss: 5.4336734
[285] loss: 5.5309078
[286] loss: 5.3071612
[287] loss: 5.2858330
[288] loss: 5.4671373
[289] loss: 5.4525537
[290] loss: 5.3744032
[291] loss: 5.4524489
[292] loss: 5.3317983
[293] loss: 5.3346627
[294] loss: 5.4068687
[295] loss: 5.2458318
[296] loss: 5.3401151
[297] loss: 5.2794308
[298] loss: 5.3120593
[299] loss: 5.3774789
[300] loss: 5.3102735
[301] loss: 5.3304769
[302] loss: 5.2833151
[303] loss: 5.2518045
[304] loss: 5.3355848
[305] loss: 5.3780699
[306] loss: 5.3195086
[307] loss: 5.3816276
[308] loss: 5.3156987
[309] loss: 5.2977292
[310] loss: 5.1295199
[311] loss: 5.3225942
[312] loss: 5.2219237
[313] loss: 5.2909452
[314] loss: 5.2763734
[315] loss: 5.2614942
[316] loss: 5.2347321
[317] loss: 5.2839648
[318] loss: 5.3191450
[319] loss: 5.2168012
[320] loss: 5.2954229
[321] loss: 5.2315105
[322] loss: 5.2616969
[323] loss: 5.1743340
[324] loss: 5.2899686
[325] loss: 5.2076844
[326] loss: 5.2646072
[327] loss: 5.2036807
[328] loss: 5.1549292
[329] loss: 5.2589475
[330] loss: 5.1062176
[331] loss: 5.2450330
[332] loss: 5.1554272
[333] loss: 5.2203426
[334] loss: 5.2777089
[335] loss: 5.2109709
[336] loss: 5.1797061
[337] loss: 5.2398589
[338] loss: 5.2052868
[339] loss: 5.1735030
[340] loss: 5.0392499
[341] loss: 5.1382107
[342] loss: 5.2218200
[343] loss: 5.1365326
[344] loss: 5.0528020
[345] loss: 5.2201154
[346] loss: 5.1681781
[347] loss: 5.2168473
[348] loss: 5.1472041
[349] loss: 5.0429952
[350] loss: 5.1445020
[351] loss: 5.0916874
[352] loss: 5.0805876
[353] loss: 5.0082965
[354] loss: 5.0979837
[355] loss: 5.1508691
[356] loss: 5.0742214
[357] loss: 4.9416777
[358] loss: 4.9773620
[359] loss: 5.1661904
[360] loss: 5.0994387
[361] loss: 4.9754163
[362] loss: 5.2539654
[363] loss: 5.2709827
[364] loss: 5.1524097
[365] loss: 4.9963773
[366] loss: 4.9951159
[367] loss: 5.0991890
Finished Training
The r2m value for this run is:  0.654
The AUPR for this run is:  0.704
The Concordance Index (CI) for this run is:  0.875
The Mean Squared Error (MSE) for this run is:  0.245
r2m std is:  0.011
AUPR std is:  0.014
CIs std is:  0.004
Best parameters were: [216, 0.0001, 367]


PS:

Read file <stderr_davis.log> for stderr output of this job.

