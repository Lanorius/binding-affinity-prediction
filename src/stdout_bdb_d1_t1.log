Sender: LSF System <lsfadmin@lsf-server-3.rostclust>
Subject: Job 14205: <dg_prediction_bdb_d1_t1> in cluster <rost_lsf_cluster_1> Done

Job <dg_prediction_bdb_d1_t1> was submitted from host <lsf-master-1.rostclust> by user <giessing> in cluster <rost_lsf_cluster_1> at Thu Jan 27 15:58:40 2022
Job was executed on host(s) <lsf-server-3.rostclust>, in queue <low-end-normal>, as user <giessing> in cluster <rost_lsf_cluster_1> at Thu Jan 27 15:58:40 2022
</mnt/home/giessing> was used as the home directory.
</mnt/project/protdrugaffinity/dg/binding-affinity-prediction/src> was used as the working directory.
Started at Thu Jan 27 15:58:40 2022
Terminated at Fri Jan 28 02:10:59 2022
Results reported at Fri Jan 28 02:10:59 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python binding_prediction.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   142406.94 sec.
    Max Memory :                                 1674 MB
    Average Memory :                             1660.02 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                26
    Run time :                                   36739 sec.
    Turnaround time :                            36739 sec.

The output (if any) follows:

Using device: cuda
Finished Tuning
[256, 0.0001, 360]
Training attempt No: 1
[1] loss: 23.4044731
[2] loss: 20.5574285
[3] loss: 19.1146501
[4] loss: 18.4386647
[5] loss: 17.7845986
[6] loss: 17.2889716
[7] loss: 16.8730763
[8] loss: 16.5464029
[9] loss: 16.4612457
[10] loss: 16.1413425
[11] loss: 15.7546084
[12] loss: 15.4586918
[13] loss: 15.3616812
[14] loss: 14.9535956
[15] loss: 14.7437561
[16] loss: 14.8503529
[17] loss: 14.4066621
[18] loss: 14.3791421
[19] loss: 14.2501351
[20] loss: 14.3048931
[21] loss: 14.2039997
[22] loss: 13.9040554
[23] loss: 13.6560045
[24] loss: 13.7504286
[25] loss: 13.4277056
[26] loss: 13.4393170
[27] loss: 13.2424880
[28] loss: 13.4156867
[29] loss: 14.1053315
[30] loss: 13.5310639
[31] loss: 13.4647222
[32] loss: 13.0385651
[33] loss: 12.9968959
[34] loss: 12.8214587
[35] loss: 13.0123009
[36] loss: 13.0522097
[37] loss: 12.8205932
[38] loss: 12.9075096
[39] loss: 12.7919749
[40] loss: 12.8875851
[41] loss: 12.8625873
[42] loss: 12.6466414
[43] loss: 12.6205131
[44] loss: 12.7852271
[45] loss: 12.5776444
[46] loss: 12.5559013
[47] loss: 12.7932571
[48] loss: 12.6819074
[49] loss: 12.6922038
[50] loss: 12.5221933
[51] loss: 12.7078848
[52] loss: 12.7532333
[53] loss: 12.6321945
[54] loss: 12.7127696
[55] loss: 12.2811374
[56] loss: 12.5402178
[57] loss: 12.1258628
[58] loss: 12.3503683
[59] loss: 12.6182415
[60] loss: 12.3780229
[61] loss: 12.3488855
[62] loss: 12.2177224
[63] loss: 12.1992371
[64] loss: 12.5354752
[65] loss: 12.5071766
[66] loss: 12.5368231
[67] loss: 12.1954052
[68] loss: 12.0438626
[69] loss: 12.1394838
[70] loss: 12.2620078
[71] loss: 12.4610769
[72] loss: 12.5782277
[73] loss: 12.2673561
[74] loss: 12.3119967
[75] loss: 12.2304583
[76] loss: 12.2221088
[77] loss: 12.1718903
[78] loss: 12.1209225
[79] loss: 12.0971653
[80] loss: 12.4752910
[81] loss: 12.3423896
[82] loss: 12.1622094
[83] loss: 12.3994880
[84] loss: 11.9623181
[85] loss: 12.1672581
[86] loss: 12.1924950
[87] loss: 12.0625190
[88] loss: 12.0456868
[89] loss: 11.9013253
[90] loss: 11.9172363
[91] loss: 13.1291434
[92] loss: 15.5583618
[93] loss: 14.1683256
[94] loss: 12.7060781
[95] loss: 12.3067562
[96] loss: 13.0655310
[97] loss: 12.7308024
[98] loss: 12.3768436
[99] loss: 12.9845563
[100] loss: 12.7816576
[101] loss: 12.5465925
[102] loss: 12.6328481
[103] loss: 12.2421112
[104] loss: 12.4861137
[105] loss: 12.0546909
[106] loss: 12.4191450
[107] loss: 12.1332637
[108] loss: 12.2943277
[109] loss: 12.0163175
[110] loss: 12.6103429
[111] loss: 12.1434936
[112] loss: 11.6919543
[113] loss: 12.1170877
[114] loss: 12.0766501
[115] loss: 11.7255647
[116] loss: 11.6671203
[117] loss: 11.7653640
[118] loss: 11.7457892
[119] loss: 11.7196572
[120] loss: 11.8193732
[121] loss: 11.6429432
[122] loss: 11.6783720
[123] loss: 11.6180364
[124] loss: 11.6472109
[125] loss: 11.7443539
[126] loss: 11.7346119
[127] loss: 11.6467620
[128] loss: 11.7306951
[129] loss: 11.5012976
[130] loss: 11.6272120
[131] loss: 11.7885903
[132] loss: 11.7427072
[133] loss: 11.5208441
[134] loss: 11.5728662
[135] loss: 11.5825093
[136] loss: 11.6579914
[137] loss: 11.7752524
[138] loss: 11.6834076
[139] loss: 11.7412654
[140] loss: 11.9632487
[141] loss: 12.1147360
[142] loss: 12.1816599
[143] loss: 11.8153485
[144] loss: 14.7295430
[145] loss: 14.4437311
[146] loss: 13.2100637
[147] loss: 11.8159898
[148] loss: 11.9334906
[149] loss: 11.4624990
[150] loss: 11.6985214
[151] loss: 11.9702526
[152] loss: 12.4122942
[153] loss: 12.4300380
[154] loss: 12.1151485
[155] loss: 11.5740239
[156] loss: 11.5162731
[157] loss: 11.3982404
[158] loss: 11.3696526
[159] loss: 11.4816471
[160] loss: 11.3757379
[161] loss: 11.3995932
[162] loss: 11.3772894
[163] loss: 11.4816005
[164] loss: 11.5254748
[165] loss: 11.4815155
[166] loss: 11.4321572
[167] loss: 11.7601255
[168] loss: 11.3994770
[169] loss: 11.5091986
[170] loss: 11.4779106
[171] loss: 11.6893743
[172] loss: 11.3543922
[173] loss: 11.3997659
[174] loss: 11.5104811
[175] loss: 11.4301995
[176] loss: 11.4608827
[177] loss: 11.5196253
[178] loss: 11.5852358
[179] loss: 11.3754705
[180] loss: 11.5194359
[181] loss: 11.4662583
[182] loss: 11.2434974
[183] loss: 11.5246550
[184] loss: 11.4376328
[185] loss: 11.4110989
[186] loss: 12.5266749
[187] loss: 12.0045488
[188] loss: 11.9094645
[189] loss: 12.2350328
[190] loss: 11.8980075
[191] loss: 11.4087652
[192] loss: 11.6863196
[193] loss: 12.1060808
[194] loss: 11.4098343
[195] loss: 11.2877953
[196] loss: 11.9537694
[197] loss: 11.8881160
[198] loss: 11.3296955
[199] loss: 11.9298721
[200] loss: 11.4243388
[201] loss: 11.4855669
[202] loss: 11.3970137
[203] loss: 11.2864269
[204] loss: 11.3341947
[205] loss: 11.2416106
[206] loss: 11.5054397
[207] loss: 11.6044210
[208] loss: 11.4045476
[209] loss: 11.6598471
[210] loss: 11.8657447
[211] loss: 11.7187238
[212] loss: 12.6163513
[213] loss: 12.2884202
[214] loss: 12.0293447
[215] loss: 12.1807333
[216] loss: 12.1977190
[217] loss: 11.9441151
[218] loss: 11.7800438
[219] loss: 11.6825837
[220] loss: 11.4717063
[221] loss: 11.5383277
[222] loss: 12.1070387
[223] loss: 14.0310960
[224] loss: 14.4784343
[225] loss: 15.5144502
[226] loss: 13.6595985
[227] loss: 11.7633267
[228] loss: 11.3254412
[229] loss: 11.3390489
[230] loss: 11.3410191
[231] loss: 13.0556753
[232] loss: 13.8471744
[233] loss: 12.2730385
[234] loss: 11.8648928
[235] loss: 11.7226625
[236] loss: 11.8591613
[237] loss: 11.5140895
[238] loss: 11.6441300
[239] loss: 11.3356187
[240] loss: 11.3200546
[241] loss: 11.3931677
[242] loss: 11.4934912
[243] loss: 11.3238878
[244] loss: 11.4869093
[245] loss: 11.4094815
[246] loss: 11.4227528
[247] loss: 11.3699272
[248] loss: 11.3460865
[249] loss: 11.5138206
[250] loss: 11.5632983
[251] loss: 11.1451826
[252] loss: 11.3752070
[253] loss: 11.3397199
[254] loss: 11.4266987
[255] loss: 11.2661334
[256] loss: 11.5741326
[257] loss: 11.5087742
[258] loss: 11.1966798
[259] loss: 11.3884230
[260] loss: 11.4133928
[261] loss: 11.4245396
[262] loss: 11.5651113
[263] loss: 11.4404471
[264] loss: 11.9170156
[265] loss: 11.7572602
[266] loss: 11.8121518
[267] loss: 11.8091416
[268] loss: 11.4362368
[269] loss: 11.4570388
[270] loss: 11.1324810
[271] loss: 11.2499011
[272] loss: 11.2784701
[273] loss: 11.5151983
[274] loss: 11.3764876
[275] loss: 11.6278687
[276] loss: 11.2162832
[277] loss: 11.4967000
[278] loss: 11.5930913
[279] loss: 12.7405939
[280] loss: 12.9525188
[281] loss: 11.3967180
[282] loss: 11.4797476
[283] loss: 11.4998330
[284] loss: 11.9734872
[285] loss: 11.8584173
[286] loss: 11.3809499
[287] loss: 11.4736394
[288] loss: 11.2846391
[289] loss: 11.5168310
[290] loss: 11.5936865
[291] loss: 11.9096870
[292] loss: 12.4328087
[293] loss: 13.3859075
[294] loss: 14.9785876
[295] loss: 13.7166346
[296] loss: 11.7339381
[297] loss: 11.6800465
[298] loss: 11.1184870
[299] loss: 11.2302987
[300] loss: 11.4787140
[301] loss: 11.3413091
[302] loss: 11.3717680
[303] loss: 11.3124667
[304] loss: 11.2493793
[305] loss: 11.3462603
[306] loss: 11.3801609
[307] loss: 11.2028331
[308] loss: 11.4269754
[309] loss: 11.2453715
[310] loss: 11.1595386
[311] loss: 11.4770015
[312] loss: 11.5511411
[313] loss: 11.3862932
[314] loss: 11.3465217
[315] loss: 11.5055167
[316] loss: 11.4029759
[317] loss: 11.4700909
[318] loss: 11.3576406
[319] loss: 11.3151930
[320] loss: 11.6023250
[321] loss: 11.4271551
[322] loss: 11.4619609
[323] loss: 11.4177558
[324] loss: 11.3602376
[325] loss: 11.1583859
[326] loss: 11.2800246
[327] loss: 11.4565590
[328] loss: 11.3592413
[329] loss: 11.5485114
[330] loss: 11.2614459
[331] loss: 11.6904442
[332] loss: 12.0509187
[333] loss: 12.4422233
[334] loss: 12.8873406
[335] loss: 13.7974222
[336] loss: 11.9178503
[337] loss: 11.1665000
[338] loss: 11.8974670
[339] loss: 12.8872310
[340] loss: 13.1060461
[341] loss: 13.3672850
[342] loss: 12.3051093
[343] loss: 11.4443658
[344] loss: 11.3265059
[345] loss: 11.4328586
[346] loss: 11.5284060
[347] loss: 11.4104121
[348] loss: 11.0698038
[349] loss: 11.1593086
[350] loss: 11.3730746
[351] loss: 11.5910269
[352] loss: 11.3763034
[353] loss: 11.7293490
[354] loss: 11.4238644
[355] loss: 11.5379240
[356] loss: 12.0984792
[357] loss: 11.5934904
[358] loss: 11.6202053
[359] loss: 11.7681313
[360] loss: 11.6936715
Finished Training
The r2m value for this run is:  0.434
The AUPR for this run is:  0.772
The Concordance Index (CI) for this run is:  0.765
The Mean Squared Error (MSE) for this run is:  0.894
r2m std is:  0.008
AUPR std is:  0.007
CI std is:  0.002
MSE std is:  0.014
Best parameters were: [256, 0.0001, 360]


PS:

Read file <stderr_bdb_d1_t1.log> for stderr output of this job.

