Sender: LSF System <lsfadmin@lsf-server-3.rostclust>
Subject: Job 13519: <dg_prediction_davis_drug_shuff> in cluster <rost_lsf_cluster_1> Done

Job <dg_prediction_davis_drug_shuff> was submitted from host <lsf-master-1.rostclust> by user <giessing> in cluster <rost_lsf_cluster_1> at Tue Jan  4 14:38:53 2022
Job was executed on host(s) <lsf-server-3.rostclust>, in queue <low-end-normal>, as user <giessing> in cluster <rost_lsf_cluster_1> at Sat Jan  8 04:02:34 2022
</mnt/home/giessing> was used as the home directory.
</mnt/project/protdrugaffinity/dg/binding-affinity-prediction/src> was used as the working directory.
Started at Sat Jan  8 04:02:34 2022
Terminated at Mon Jan 10 23:16:34 2022
Results reported at Mon Jan 10 23:16:34 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python binding_prediction.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   750470.62 sec.
    Max Memory :                                 1679 MB
    Average Memory :                             1675.44 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                26
    Run time :                                   242039 sec.
    Turnaround time :                            549461 sec.

The output (if any) follows:

(['pkd', 'Davis'], 'chemVAE', <Section: INPUT FILES>, True, 1, True, False)
Using device: cuda
[205, 0.001, 204]
[230, 0.001, 211]
[55, 0.001, 151]
[55, 0.001, 151]
[55, 0.001, 151]
[55, 0.001, 151]
[55, 0.001, 151]
[55, 0.001, 151]
[55, 0.001, 151]
[55, 0.001, 151]
[55, 0.001, 151]
[55, 0.001, 151]
[55, 0.001, 151]
[55, 0.001, 151]
[55, 0.001, 151]
[55, 0.001, 151]
[55, 0.001, 151]
[55, 0.001, 151]
[55, 0.001, 151]
[55, 0.001, 151]
[55, 0.001, 151]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
[30, 0.001, 274]
Finished Tuning
0.607118059790284
[30, 0.001, 274]
[1] loss: 730.4758243
[2] loss: 562.5001737
[3] loss: 523.5640195
[4] loss: 486.4639957
[5] loss: 473.4275473
[6] loss: 447.2290885
[7] loss: 436.0945625
[8] loss: 415.8661870
[9] loss: 403.9128155
[10] loss: 389.8231727
[11] loss: 377.7885844
[12] loss: 371.8708831
[13] loss: 364.7994818
[14] loss: 359.2071034
[15] loss: 358.9141357
[16] loss: 349.1778065
[17] loss: 343.1119082
[18] loss: 338.3408506
[19] loss: 337.4858094
[20] loss: 330.4338660
[21] loss: 327.6504198
[22] loss: 323.1552652
[23] loss: 324.1693214
[24] loss: 313.6809059
[25] loss: 314.1799405
[26] loss: 314.7764535
[27] loss: 309.4182193
[28] loss: 307.0610653
[29] loss: 302.9231745
[30] loss: 300.5206898
[31] loss: 299.1403593
[32] loss: 294.8563091
[33] loss: 292.9360274
[34] loss: 288.8672571
[35] loss: 286.1970299
[36] loss: 288.5090392
[37] loss: 282.1168896
[38] loss: 280.7954273
[39] loss: 275.2314659
[40] loss: 276.6663816
[41] loss: 273.6193800
[42] loss: 269.8524001
[43] loss: 269.7724386
[44] loss: 266.7200744
[45] loss: 263.9133269
[46] loss: 266.0838723
[47] loss: 261.9337514
[48] loss: 263.1393365
[49] loss: 260.7753515
[50] loss: 260.0765549
[51] loss: 257.8687978
[52] loss: 258.7892213
[53] loss: 256.2347972
[54] loss: 256.3074108
[55] loss: 253.8225690
[56] loss: 251.9564353
[57] loss: 249.8067620
[58] loss: 249.0835509
[59] loss: 249.8813107
[60] loss: 250.5762203
[61] loss: 248.2572698
[62] loss: 246.7817614
[63] loss: 245.6834276
[64] loss: 241.8892843
[65] loss: 243.7049998
[66] loss: 242.9964670
[67] loss: 241.7347218
[68] loss: 239.3516112
[69] loss: 240.1424456
[70] loss: 239.1832426
[71] loss: 235.1871239
[72] loss: 233.2476905
[73] loss: 234.1804977
[74] loss: 232.5221531
[75] loss: 233.6869578
[76] loss: 231.6886225
[77] loss: 230.7623646
[78] loss: 230.1291981
[79] loss: 226.6713576
[80] loss: 230.6792029
[81] loss: 225.5125627
[82] loss: 224.0868574
[83] loss: 223.2499963
[84] loss: 222.2443004
[85] loss: 222.6649837
[86] loss: 217.6828131
[87] loss: 221.0578270
[88] loss: 218.6620755
[89] loss: 218.3603899
[90] loss: 217.3388406
[91] loss: 218.0630795
[92] loss: 217.1961047
[93] loss: 217.1833082
[94] loss: 214.3539506
[95] loss: 213.6361007
[96] loss: 213.2027827
[97] loss: 212.8199885
[98] loss: 212.7764891
[99] loss: 213.0098704
[100] loss: 212.2026589
[101] loss: 210.7429985
[102] loss: 210.2373414
[103] loss: 210.4682283
[104] loss: 208.0627674
[105] loss: 206.4424400
[106] loss: 209.9392985
[107] loss: 207.6068357
[108] loss: 207.3927491
[109] loss: 206.2513306
[110] loss: 208.6681466
[111] loss: 207.0047430
[112] loss: 205.2180245
[113] loss: 203.7275729
[114] loss: 203.8394359
[115] loss: 203.1849075
[116] loss: 202.0509364
[117] loss: 203.1182268
[118] loss: 202.4098075
[119] loss: 201.3685100
[120] loss: 202.3787372
[121] loss: 201.6974922
[122] loss: 200.1270218
[123] loss: 202.3129281
[124] loss: 199.7201795
[125] loss: 201.5127530
[126] loss: 200.0004951
[127] loss: 198.3164561
[128] loss: 198.0141759
[129] loss: 196.6410738
[130] loss: 194.8680402
[131] loss: 195.7769236
[132] loss: 196.9850080
[133] loss: 195.3586414
[134] loss: 196.0760429
[135] loss: 195.3982869
[136] loss: 193.3121041
[137] loss: 192.7741064
[138] loss: 194.9446652
[139] loss: 192.5983265
[140] loss: 194.8227098
[141] loss: 190.5635384
[142] loss: 190.9957286
[143] loss: 191.7807599
[144] loss: 191.1387818
[145] loss: 190.8738704
[146] loss: 191.0052044
[147] loss: 189.8641605
[148] loss: 189.0020711
[149] loss: 190.2331572
[150] loss: 188.4390394
[151] loss: 187.8867519
[152] loss: 188.8005708
[153] loss: 187.1675729
[154] loss: 188.1504187
[155] loss: 188.1292520
[156] loss: 188.2853816
[157] loss: 186.7972448
[158] loss: 186.7532078
[159] loss: 184.7872961
[160] loss: 185.3269201
[161] loss: 185.9720549
[162] loss: 185.6360107
[163] loss: 186.3714748
[164] loss: 184.6324262
[165] loss: 182.2660040
[166] loss: 185.3855069
[167] loss: 182.3232237
[168] loss: 184.7578713
[169] loss: 183.6146006
[170] loss: 182.9997183
[171] loss: 181.7501614
[172] loss: 184.4996542
[173] loss: 182.9070724
[174] loss: 182.8589063
[175] loss: 181.4500445
[176] loss: 180.3306881
[177] loss: 181.7327762
[178] loss: 180.3198159
[179] loss: 180.8240449
[180] loss: 179.4408221
[181] loss: 178.8694450
[182] loss: 179.8490837
[183] loss: 178.9337997
[184] loss: 179.4637513
[185] loss: 179.8401926
[186] loss: 179.2600149
[187] loss: 176.9131879
[188] loss: 176.2329608
[189] loss: 176.7801311
[190] loss: 178.2355669
[191] loss: 175.0761606
[192] loss: 175.5135960
[193] loss: 175.3850431
[194] loss: 175.5299606
[195] loss: 174.3851892
[196] loss: 176.2421732
[197] loss: 173.6037364
[198] loss: 174.0365096
[199] loss: 174.0043622
[200] loss: 173.3359273
[201] loss: 174.0231825
[202] loss: 171.4677943
[203] loss: 171.7655064
[204] loss: 170.1369017
[205] loss: 171.6501274
[206] loss: 169.2021560
[207] loss: 171.2342242
[208] loss: 171.2622045
[209] loss: 171.9293096
[210] loss: 172.2644064
[211] loss: 170.5078789
[212] loss: 169.3981656
[213] loss: 168.9966631
[214] loss: 169.1855848
[215] loss: 169.3712176
[216] loss: 169.2458489
[217] loss: 168.2892424
[218] loss: 167.0621506
[219] loss: 168.4867849
[220] loss: 166.2347609
[221] loss: 168.5124073
[222] loss: 167.2959865
[223] loss: 166.3704352
[224] loss: 165.8509880
[225] loss: 168.5236552
[226] loss: 166.2761767
[227] loss: 165.1353069
[228] loss: 166.4819214
[229] loss: 164.6327565
[230] loss: 162.4224914
[231] loss: 163.5693838
[232] loss: 165.5257490
[233] loss: 163.3668192
[234] loss: 164.9822125
[235] loss: 163.4987861
[236] loss: 162.1890845
[237] loss: 162.3768672
[238] loss: 162.7288041
[239] loss: 161.0777709
[240] loss: 161.0991725
[241] loss: 162.1345902
[242] loss: 160.8275375
[243] loss: 160.7453445
[244] loss: 160.9222496
[245] loss: 159.6462066
[246] loss: 158.4436802
[247] loss: 161.4425079
[248] loss: 158.1085278
[249] loss: 159.5181426
[250] loss: 159.9981843
[251] loss: 158.7561326
[252] loss: 160.0347720
[253] loss: 157.5777116
[254] loss: 158.5621123
[255] loss: 155.0094065
[256] loss: 156.3474502
[257] loss: 157.8654790
[258] loss: 158.0630614
[259] loss: 155.4428196
[260] loss: 154.9102381
[261] loss: 154.3288103
[262] loss: 155.7044842
[263] loss: 157.7110857
[264] loss: 155.8333594
[265] loss: 157.1375923
[266] loss: 153.9198681
[267] loss: 154.1487561
[268] loss: 155.8636269
[269] loss: 151.2821546
[270] loss: 153.5561755
[271] loss: 155.1593515
[272] loss: 153.9072194
[273] loss: 152.1338537
[274] loss: 150.5210402
Finished Training
The r2m value for this run is:  0.613
The AUPR for this run is:  0.689
The Concordance Index (CI) for this run is:  0.866
The Mean Squared Error (MSE) for this run is:  0.281
r2m std is:  0.011
AUPR std is:  0.017
CIs std is:  0.004
Best parameters were: [30, 0.001, 274]


PS:

Read file <stderr_davis_drug_shuff.log> for stderr output of this job.

