Sender: LSF System <lsfadmin@lsf-server-3.rostclust>
Subject: Job 13096: <dg_prediction_pkd_d08_p08> in cluster <rost_lsf_cluster_1> Done

Job <dg_prediction_pkd_d08_p08> was submitted from host <lsf-master-1.rostclust> by user <giessing> in cluster <rost_lsf_cluster_1> at Fri Dec 10 09:50:04 2021
Job was executed on host(s) <lsf-server-3.rostclust>, in queue <low-end-normal>, as user <giessing> in cluster <rost_lsf_cluster_1> at Fri Dec 10 09:50:04 2021
</mnt/home/giessing> was used as the home directory.
</mnt/project/protdrugaffinity/dg/binding-affinity-prediction/src> was used as the working directory.
Started at Fri Dec 10 09:50:04 2021
Terminated at Sat Dec 11 03:16:51 2021
Results reported at Sat Dec 11 03:16:51 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python binding_prediction.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   154244.00 sec.
    Max Memory :                                 1722 MB
    Average Memory :                             1645.88 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                26
    Run time :                                   62806 sec.
    Turnaround time :                            62807 sec.

The output (if any) follows:

(['pkd', 'Davis'], 'RDKit', <Section: INPUT FILES>, True, 1, True, True)
Using device: cuda
Finished Tuning
0.18727272464769296
[90, 0.006200000000000001, 297]
[1] loss: 42094.2020693
[2] loss: 215.8447621
[3] loss: 216.7166438
[4] loss: 205.4302709
[5] loss: 204.1616661
[6] loss: 195.1030415
[7] loss: 187.6302626
[8] loss: 181.4650569
[9] loss: 179.0722640
[10] loss: 176.1995500
[11] loss: 174.2824426
[12] loss: 172.4363925
[13] loss: 175.4431306
[14] loss: 174.0793873
[15] loss: 170.6198816
[16] loss: 172.0019014
[17] loss: 168.9999387
[18] loss: 168.3957701
[19] loss: 169.4386306
[20] loss: 167.5263810
[21] loss: 166.6693895
[22] loss: 168.2558015
[23] loss: 169.6453267
[24] loss: 166.4579754
[25] loss: 166.3499832
[26] loss: 165.6997877
[27] loss: 164.6924527
[28] loss: 171.4807658
[29] loss: 166.1565416
[30] loss: 164.3628463
[31] loss: 165.2410669
[32] loss: 162.9626455
[33] loss: 162.6977183
[34] loss: 165.4451569
[35] loss: 162.3990333
[36] loss: 160.7981615
[37] loss: 161.2162343
[38] loss: 162.3566757
[39] loss: 162.9867205
[40] loss: 161.7538693
[41] loss: 158.7278584
[42] loss: 158.8255157
[43] loss: 160.5082369
[44] loss: 158.0625065
[45] loss: 157.6596172
[46] loss: 157.6378980
[47] loss: 156.6520405
[48] loss: 157.4875054
[49] loss: 157.0178789
[50] loss: 155.0819258
[51] loss: 154.2953315
[52] loss: 154.2948652
[53] loss: 154.2124383
[54] loss: 154.4235198
[55] loss: 154.1483703
[56] loss: 154.2760168
[57] loss: 153.7472206
[58] loss: 152.4989129
[59] loss: 153.1067515
[60] loss: 153.3502366
[61] loss: 154.7515540
[62] loss: 152.8893136
[63] loss: 155.0271652
[64] loss: 152.6270508
[65] loss: 153.6362733
[66] loss: 151.4988977
[67] loss: 152.7686253
[68] loss: 152.8857196
[69] loss: 152.4292076
[70] loss: 152.1494157
[71] loss: 152.3331684
[72] loss: 150.9176169
[73] loss: 151.3179717
[74] loss: 153.7102129
[75] loss: 150.8148456
[76] loss: 150.8095114
[77] loss: 150.2858595
[78] loss: 149.0032756
[79] loss: 150.2576261
[80] loss: 151.8395449
[81] loss: 150.2538721
[82] loss: 149.0484319
[83] loss: 149.7724811
[84] loss: 148.3830581
[85] loss: 148.7253534
[86] loss: 148.1312113
[87] loss: 148.2657430
[88] loss: 147.2651912
[89] loss: 148.9998267
[90] loss: 148.9786230
[91] loss: 148.0309001
[92] loss: 148.1675032
[93] loss: 148.4409100
[94] loss: 148.8362335
[95] loss: 149.1015231
[96] loss: 148.2606876
[97] loss: 148.2519617
[98] loss: 148.8404674
[99] loss: 146.9505586
[100] loss: 147.3700446
[101] loss: 147.0222818
[102] loss: 147.2744932
[103] loss: 148.5645239
[104] loss: 149.0042384
[105] loss: 147.9777842
[106] loss: 147.8825580
[107] loss: 147.9790954
[108] loss: 149.0900082
[109] loss: 148.4364069
[110] loss: 148.2646309
[111] loss: 147.7436710
[112] loss: 148.5220488
[113] loss: 146.6790333
[114] loss: 146.4043263
[115] loss: 147.0392500
[116] loss: 146.5164560
[117] loss: 145.9765274
[118] loss: 146.2385190
[119] loss: 146.7546017
[120] loss: 145.9579951
[121] loss: 146.0881685
[122] loss: 145.8598863
[123] loss: 146.5359844
[124] loss: 145.9007828
[125] loss: 145.6313166
[126] loss: 145.8329498
[127] loss: 145.7692477
[128] loss: 145.6784823
[129] loss: 145.6069148
[130] loss: 146.3774028
[131] loss: 145.9576432
[132] loss: 144.8764453
[133] loss: 145.5360097
[134] loss: 146.8502311
[135] loss: 145.8330479
[136] loss: 144.9025003
[137] loss: 144.2110649
[138] loss: 144.8999272
[139] loss: 144.7414331
[140] loss: 144.5551740
[141] loss: 144.6837392
[142] loss: 144.6428850
[143] loss: 144.9661318
[144] loss: 143.3414228
[145] loss: 143.7475946
[146] loss: 144.2086893
[147] loss: 143.9672771
[148] loss: 144.1225423
[149] loss: 143.5809323
[150] loss: 145.1778495
[151] loss: 144.5035341
[152] loss: 144.0359293
[153] loss: 144.5029377
[154] loss: 144.2627517
[155] loss: 144.3442644
[156] loss: 145.5947677
[157] loss: 145.4317701
[158] loss: 144.3032318
[159] loss: 147.1968248
[160] loss: 147.0951903
[161] loss: 145.1868781
[162] loss: 146.4596521
[163] loss: 147.4082868
[164] loss: 144.2084425
[165] loss: 144.4660858
[166] loss: 144.1178917
[167] loss: 145.1704667
[168] loss: 145.7786591
[169] loss: 143.7370440
[170] loss: 143.6324238
[171] loss: 143.8466680
[172] loss: 143.1178864
[173] loss: 143.4180170
[174] loss: 143.5941543
[175] loss: 145.4224066
[176] loss: 147.4746569
[177] loss: 147.4373758
[178] loss: 146.1750989
[179] loss: 145.1061360
[180] loss: 143.9237313
[181] loss: 144.5286361
[182] loss: 143.8082033
[183] loss: 143.8445944
[184] loss: 143.1320026
[185] loss: 143.9101668
[186] loss: 144.2355163
[187] loss: 144.5802027
[188] loss: 143.6156272
[189] loss: 144.2972306
[190] loss: 143.9333364
[191] loss: 145.2091781
[192] loss: 145.0585723
[193] loss: 142.8942683
[194] loss: 144.3482946
[195] loss: 142.6844831
[196] loss: 143.2314727
[197] loss: 141.5167126
[198] loss: 143.1125239
[199] loss: 142.7647787
[200] loss: 142.4170634
[201] loss: 143.2364365
[202] loss: 142.5965058
[203] loss: 143.1037453
[204] loss: 144.0959168
[205] loss: 142.9856553
[206] loss: 143.5527812
[207] loss: 142.2054768
[208] loss: 142.6171457
[209] loss: 142.6914385
[210] loss: 142.0636045
[211] loss: 142.2626444
[212] loss: 144.0329654
[213] loss: 143.9806498
[214] loss: 144.6037258
[215] loss: 143.8489371
[216] loss: 143.6120424
[217] loss: 143.7219092
[218] loss: 143.0296204
[219] loss: 143.1879459
[220] loss: 142.3303959
[221] loss: 142.0771055
[222] loss: 142.7828507
[223] loss: 142.5307393
[224] loss: 142.3240243
[225] loss: 142.7390341
[226] loss: 142.4527571
[227] loss: 142.0691209
[228] loss: 141.6513141
[229] loss: 141.3356841
[230] loss: 141.1493049
[231] loss: 141.9027355
[232] loss: 140.9359448
[233] loss: 141.9523095
[234] loss: 141.7841484
[235] loss: 141.5182305
[236] loss: 140.5618052
[237] loss: 142.2969341
[238] loss: 143.6974247
[239] loss: 142.3677291
[240] loss: 143.8667733
[241] loss: 142.6010153
[242] loss: 141.4892405
[243] loss: 141.5215174
[244] loss: 141.2703733
[245] loss: 142.6167045
[246] loss: 142.6479410
[247] loss: 142.2498326
[248] loss: 143.0846175
[249] loss: 141.3756517
[250] loss: 142.0667818
[251] loss: 144.4721384
[252] loss: 141.8213704
[253] loss: 141.4710200
[254] loss: 141.5647867
[255] loss: 140.3420671
[256] loss: 141.4403859
[257] loss: 140.6123328
[258] loss: 141.4260923
[259] loss: 141.2782479
[260] loss: 142.9497100
[261] loss: 140.8565270
[262] loss: 140.4036069
[263] loss: 141.8475467
[264] loss: 141.3423234
[265] loss: 140.8635163
[266] loss: 141.0276105
[267] loss: 140.8125797
[268] loss: 141.0911004
[269] loss: 140.6707347
[270] loss: 141.0286970
[271] loss: 141.6491955
[272] loss: 141.3684632
[273] loss: 140.9391953
[274] loss: 142.7075145
[275] loss: 145.3372062
[276] loss: 142.7606642
[277] loss: 141.5610459
[278] loss: 140.3285580
[279] loss: 140.6260264
[280] loss: 140.6565056
[281] loss: 142.3447716
[282] loss: 141.6271382
[283] loss: 140.5497443
[284] loss: 140.6524225
[285] loss: 140.8425488
[286] loss: 141.6437003
[287] loss: 141.0476070
[288] loss: 142.0276770
[289] loss: 142.6333327
[290] loss: 142.7934519
[291] loss: 142.3337579
[292] loss: 141.4749157
[293] loss: 140.5329194
[294] loss: 140.9828876
[295] loss: 140.9545668
[296] loss: 140.5951383
[297] loss: 141.0971489
Finished Training
The r2m value for this run is:  0.182
The AUPR for this run is:  0.582
The Concordance Index (CI) for this run is:  0.65
The Mean Squared Error (MSE) for this run is:  1.37
r2m std is:  0.005
AUPR std is:  0.01
CIs std is:  0.002
Best r2m was:  0.18727272464769296
Best parameters were: [90, 0.006200000000000001, 297]


PS:

Read file <stderr_pkd_d08_p08.log> for stderr output of this job.

