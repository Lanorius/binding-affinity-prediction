Sender: LSF System <lsfadmin@lsf-server-3.rostclust>
Subject: Job 13171: <dg_prediction_pkd_d1_p1> in cluster <rost_lsf_cluster_1> Done

Job <dg_prediction_pkd_d1_p1> was submitted from host <lsf-master-1.rostclust> by user <giessing> in cluster <rost_lsf_cluster_1> at Thu Dec 16 08:04:48 2021
Job was executed on host(s) <lsf-server-3.rostclust>, in queue <low-end-normal>, as user <giessing> in cluster <rost_lsf_cluster_1> at Thu Dec 16 08:04:48 2021
</mnt/home/giessing> was used as the home directory.
</mnt/project/protdrugaffinity/dg/binding-affinity-prediction/src> was used as the working directory.
Started at Thu Dec 16 08:04:48 2021
Terminated at Fri Dec 17 04:17:40 2021
Results reported at Fri Dec 17 04:17:40 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python binding_prediction.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   174443.09 sec.
    Max Memory :                                 1717 MB
    Average Memory :                             1654.32 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                26
    Run time :                                   72771 sec.
    Turnaround time :                            72772 sec.

The output (if any) follows:

(['pkd', 'Davis'], 'chemVAE', <Section: INPUT FILES>, True, 1, False, False)
Using device: cuda
[370, 0.00030000000000000003, 172]
[370, 0.00030000000000000003, 172]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
[270, 0.0074, 235]
Finished Tuning
0.3997203879973334
[270, 0.0074, 235]
[1] loss: 7808.7401970
[2] loss: 135.3427660
[3] loss: 88.2014375
[4] loss: 73.3598559
[5] loss: 67.1388138
[6] loss: 60.1891277
[7] loss: 57.0885312
[8] loss: 54.9242940
[9] loss: 52.5039951
[10] loss: 49.0427358
[11] loss: 48.2604695
[12] loss: 44.7271321
[13] loss: 45.5360682
[14] loss: 45.4584233
[15] loss: 45.7399017
[16] loss: 45.2040614
[17] loss: 40.8157099
[18] loss: 36.7256308
[19] loss: 35.2754634
[20] loss: 34.9474473
[21] loss: 34.5505685
[22] loss: 34.9091659
[23] loss: 35.1854958
[24] loss: 34.4789063
[25] loss: 34.7097970
[26] loss: 34.4298788
[27] loss: 33.1444519
[28] loss: 32.8288348
[29] loss: 31.0455415
[30] loss: 31.2246209
[31] loss: 30.5744842
[32] loss: 31.0627165
[33] loss: 30.6102718
[34] loss: 30.8452933
[35] loss: 30.7517489
[36] loss: 29.9568884
[37] loss: 30.2334106
[38] loss: 29.8481909
[39] loss: 30.0858642
[40] loss: 30.0729429
[41] loss: 29.8972161
[42] loss: 29.2574588
[43] loss: 29.1224418
[44] loss: 29.3918192
[45] loss: 30.3670990
[46] loss: 31.6732385
[47] loss: 33.1113775
[48] loss: 32.1751766
[49] loss: 28.9772555
[50] loss: 27.8016648
[51] loss: 27.0081481
[52] loss: 26.6019458
[53] loss: 26.3315131
[54] loss: 26.3320071
[55] loss: 25.7742410
[56] loss: 25.5143061
[57] loss: 25.8482931
[58] loss: 25.7434940
[59] loss: 25.5071382
[60] loss: 25.7137789
[61] loss: 25.8792870
[62] loss: 25.8878266
[63] loss: 25.8213861
[64] loss: 26.0753899
[65] loss: 27.0602959
[66] loss: 28.1959183
[67] loss: 28.7080752
[68] loss: 28.3687469
[69] loss: 27.3699921
[70] loss: 27.3294324
[71] loss: 27.3289350
[72] loss: 27.9392128
[73] loss: 27.4403517
[74] loss: 27.6734079
[75] loss: 27.6439987
[76] loss: 29.9437371
[77] loss: 30.0521358
[78] loss: 30.4909409
[79] loss: 30.9127263
[80] loss: 28.7077366
[81] loss: 29.2577351
[82] loss: 30.3661423
[83] loss: 35.7761335
[84] loss: 39.0699886
[85] loss: 35.5995955
[86] loss: 33.0090638
[87] loss: 29.6411996
[88] loss: 27.4038575
[89] loss: 26.4073108
[90] loss: 26.5430646
[91] loss: 25.9167451
[92] loss: 25.6789544
[93] loss: 26.0732453
[94] loss: 25.5318102
[95] loss: 25.5502037
[96] loss: 25.2348759
[97] loss: 25.8334042
[98] loss: 26.2978032
[99] loss: 25.9475201
[100] loss: 26.1526613
[101] loss: 25.5738998
[102] loss: 25.9083538
[103] loss: 25.7666112
[104] loss: 25.7295043
[105] loss: 25.3945866
[106] loss: 25.0189878
[107] loss: 25.1646331
[108] loss: 25.6585084
[109] loss: 25.9379922
[110] loss: 25.3883473
[111] loss: 25.5740006
[112] loss: 25.3429443
[113] loss: 25.9351366
[114] loss: 26.1498711
[115] loss: 26.5075156
[116] loss: 27.4037321
[117] loss: 27.3808408
[118] loss: 28.2717747
[119] loss: 29.4800480
[120] loss: 28.5081907
[121] loss: 28.3015235
[122] loss: 27.8635409
[123] loss: 27.9398922
[124] loss: 27.8059807
[125] loss: 29.3099116
[126] loss: 29.8621409
[127] loss: 28.8517176
[128] loss: 27.8769893
[129] loss: 27.9766933
[130] loss: 26.4617681
[131] loss: 26.7307053
[132] loss: 26.9655944
[133] loss: 28.6071563
[134] loss: 26.6266693
[135] loss: 27.2353020
[136] loss: 26.3522626
[137] loss: 25.1850366
[138] loss: 24.2306766
[139] loss: 24.0991187
[140] loss: 23.2535631
[141] loss: 23.0926348
[142] loss: 23.2455989
[143] loss: 22.9404660
[144] loss: 23.1913319
[145] loss: 23.4721486
[146] loss: 24.1047998
[147] loss: 23.6320459
[148] loss: 24.7909330
[149] loss: 25.8057067
[150] loss: 25.7154089
[151] loss: 27.5260072
[152] loss: 27.9281379
[153] loss: 27.0786076
[154] loss: 26.5684564
[155] loss: 24.9373375
[156] loss: 24.1470504
[157] loss: 24.7202227
[158] loss: 24.5571620
[159] loss: 23.2440996
[160] loss: 23.3781838
[161] loss: 23.3291491
[162] loss: 23.3755314
[163] loss: 23.1357647
[164] loss: 23.0957429
[165] loss: 22.9567338
[166] loss: 22.8189878
[167] loss: 22.8833611
[168] loss: 22.8949457
[169] loss: 24.0037520
[170] loss: 23.8628362
[171] loss: 23.1029636
[172] loss: 23.2199502
[173] loss: 23.7641769
[174] loss: 24.5487390
[175] loss: 24.6807842
[176] loss: 25.2969520
[177] loss: 24.4403216
[178] loss: 24.0852616
[179] loss: 24.6050798
[180] loss: 23.6379324
[181] loss: 23.6336514
[182] loss: 24.7610240
[183] loss: 24.3848143
[184] loss: 24.1786694
[185] loss: 25.0109143
[186] loss: 24.9709739
[187] loss: 24.9234730
[188] loss: 25.1865234
[189] loss: 24.4562624
[190] loss: 24.3416782
[191] loss: 25.6014576
[192] loss: 25.8732715
[193] loss: 27.2231991
[194] loss: 27.3226333
[195] loss: 26.7569212
[196] loss: 26.3372273
[197] loss: 27.0878399
[198] loss: 26.6325748
[199] loss: 25.6471597
[200] loss: 24.8742377
[201] loss: 25.2795827
[202] loss: 25.0738841
[203] loss: 24.6101064
[204] loss: 25.0855997
[205] loss: 24.4101240
[206] loss: 24.4141759
[207] loss: 24.4798711
[208] loss: 24.1118791
[209] loss: 24.4763826
[210] loss: 24.6011187
[211] loss: 23.7022765
[212] loss: 23.3108849
[213] loss: 23.3625840
[214] loss: 23.7081432
[215] loss: 23.9986249
[216] loss: 23.3930574
[217] loss: 22.2461038
[218] loss: 22.2493047
[219] loss: 21.9863982
[220] loss: 21.7023160
[221] loss: 21.4739850
[222] loss: 21.6129307
[223] loss: 21.6120056
[224] loss: 22.2388932
[225] loss: 21.9592407
[226] loss: 22.6324369
[227] loss: 22.8675202
[228] loss: 24.0132056
[229] loss: 24.0091359
[230] loss: 24.6098832
[231] loss: 24.6351366
[232] loss: 22.9550674
[233] loss: 22.2792468
[234] loss: 22.3939877
[235] loss: 22.2258729
Finished Training
The r2m value for this run is:  0.371
The AUPR for this run is:  0.717
The Concordance Index (CI) for this run is:  0.734
The Mean Squared Error (MSE) for this run is:  1.027
r2m std is:  0.01
AUPR std is:  0.007
CIs std is:  0.003
Best r2m was:  0.3997203879973334
Best parameters were: [270, 0.0074, 235]


PS:

Read file <stderr_pkd_d1_p1.log> for stderr output of this job.

