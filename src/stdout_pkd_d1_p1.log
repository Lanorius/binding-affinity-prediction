Sender: LSF System <lsfadmin@lsf-server-3.rostclust>
Subject: Job 13116: <dg_prediction_pkd_d1_p1> in cluster <rost_lsf_cluster_1> Done

Job <dg_prediction_pkd_d1_p1> was submitted from host <lsf-master-1.rostclust> by user <giessing> in cluster <rost_lsf_cluster_1> at Sat Dec 11 12:29:50 2021
Job was executed on host(s) <lsf-server-3.rostclust>, in queue <low-end-normal>, as user <giessing> in cluster <rost_lsf_cluster_1> at Sat Dec 11 12:29:51 2021
</mnt/home/giessing> was used as the home directory.
</mnt/project/protdrugaffinity/dg/binding-affinity-prediction/src> was used as the working directory.
Started at Sat Dec 11 12:29:51 2021
Terminated at Sun Dec 12 05:10:39 2021
Results reported at Sun Dec 12 05:10:39 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python binding_prediction.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   147702.00 sec.
    Max Memory :                                 1718 MB
    Average Memory :                             1651.62 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                26
    Run time :                                   60048 sec.
    Turnaround time :                            60049 sec.

The output (if any) follows:

(['pkd', 'Davis'], 'RDKit', <Section: INPUT FILES>, True, 1, True, True)
Using device: cuda
Finished Tuning
0.20706198944736515
[395, 0.0045000000000000005, 209]
[1] loss: 1157.3963697
[2] loss: 1166.8767864
[3] loss: 1166.8767864
[4] loss: 1166.8767864
[5] loss: 1166.8767864
[6] loss: 1166.8767864
[7] loss: 1166.8767864
[8] loss: 1166.8767864
[9] loss: 1166.8767864
[10] loss: 1166.8767864
[11] loss: 1166.8767864
[12] loss: 1166.8767864
[13] loss: 1166.8767864
[14] loss: 1166.8767864
[15] loss: 1166.8767864
[16] loss: 1166.8767864
[17] loss: 1166.8767864
[18] loss: 1166.8767864
[19] loss: 1166.8767864
[20] loss: 1166.8767864
[21] loss: 1166.8767864
[22] loss: 1166.8767864
[23] loss: 1166.8767864
[24] loss: 1166.8767864
[25] loss: 1166.8767864
[26] loss: 1166.8767864
[27] loss: 1166.8767864
[28] loss: 1166.8767864
[29] loss: 1166.8767864
[30] loss: 1166.8767864
[31] loss: 1166.8767864
[32] loss: 1166.8767864
[33] loss: 1166.8767864
[34] loss: 1166.8767864
[35] loss: 1166.8767864
[36] loss: 1166.8767864
[37] loss: 1166.8767864
[38] loss: 1166.8767864
[39] loss: 1166.8767864
[40] loss: 1166.8767864
[41] loss: 1166.8767864
[42] loss: 1166.8767864
[43] loss: 1166.8767864
[44] loss: 1166.8767864
[45] loss: 1166.8767864
[46] loss: 1166.8767864
[47] loss: 1166.8767864
[48] loss: 1166.8767864
[49] loss: 1166.8767864
[50] loss: 1166.8767864
[51] loss: 1166.8767864
[52] loss: 1166.8767864
[53] loss: 1166.8767864
[54] loss: 1166.8767864
[55] loss: 1166.8767864
[56] loss: 1166.8767864
[57] loss: 1166.8767864
[58] loss: 1166.8767864
[59] loss: 1166.8767864
[60] loss: 1166.8767864
[61] loss: 1166.8767864
[62] loss: 1166.8767864
[63] loss: 1166.8767864
[64] loss: 1166.8767864
[65] loss: 1166.8767864
[66] loss: 1166.8767864
[67] loss: 1166.8767864
[68] loss: 1166.8767864
[69] loss: 1166.8767864
[70] loss: 1166.8767864
[71] loss: 1166.8767864
[72] loss: 1166.8767864
[73] loss: 1166.8767864
[74] loss: 1166.8767864
[75] loss: 1166.8767864
[76] loss: 1166.8767864
[77] loss: 1166.8767864
[78] loss: 1166.8767864
[79] loss: 1166.8767864
[80] loss: 1166.8767864
[81] loss: 1166.8767864
[82] loss: 1166.8767864
[83] loss: 1166.8767864
[84] loss: 1166.8767864
[85] loss: 1166.8767864
[86] loss: 1166.8767864
[87] loss: 1166.8767864
[88] loss: 1166.8767864
[89] loss: 1166.8767864
[90] loss: 1166.8767864
[91] loss: 1166.8767864
[92] loss: 1166.8767864
[93] loss: 1166.8767864
[94] loss: 1166.8767864
[95] loss: 1166.8767864
[96] loss: 1166.8767864
[97] loss: 1166.8767864
[98] loss: 1166.8767864
[99] loss: 1166.8767864
[100] loss: 1166.8767864
[101] loss: 1166.8767864
[102] loss: 1166.8767864
[103] loss: 1166.8767864
[104] loss: 1166.8767864
[105] loss: 1166.8767864
[106] loss: 1166.8767864
[107] loss: 1166.8767864
[108] loss: 1166.8767864
[109] loss: 1166.8767864
[110] loss: 1166.8767864
[111] loss: 1166.8767864
[112] loss: 1166.8767864
[113] loss: 1166.8767864
[114] loss: 1166.8767864
[115] loss: 1166.8767864
[116] loss: 1166.8767864
[117] loss: 1166.8767864
[118] loss: 1166.8767864
[119] loss: 1166.8767864
[120] loss: 1166.8767864
[121] loss: 1166.8767864
[122] loss: 1166.8767864
[123] loss: 1166.8767864
[124] loss: 1166.8767864
[125] loss: 1166.8767864
[126] loss: 1166.8767864
[127] loss: 1166.8767864
[128] loss: 1166.8767864
[129] loss: 1166.8767864
[130] loss: 1166.8767864
[131] loss: 1166.8767864
[132] loss: 1166.8767864
[133] loss: 1166.8767864
[134] loss: 1166.8767864
[135] loss: 1166.8767864
[136] loss: 1166.8767864
[137] loss: 1166.8767864
[138] loss: 1166.8767864
[139] loss: 1166.8767864
[140] loss: 1166.8767864
[141] loss: 1166.8767864
[142] loss: 1166.8767864
[143] loss: 1166.8767864
[144] loss: 1166.8767864
[145] loss: 1166.8767864
[146] loss: 1166.8767864
[147] loss: 1166.8767864
[148] loss: 1166.8767864
[149] loss: 1166.8767864
[150] loss: 1166.8767864
[151] loss: 1166.8767864
[152] loss: 1166.8767864
[153] loss: 1166.8767864
[154] loss: 1166.8767864
[155] loss: 1166.8767864
[156] loss: 1166.8767864
[157] loss: 1166.8767864
[158] loss: 1166.8767864
[159] loss: 1166.8767864
[160] loss: 1166.8767864
[161] loss: 1166.8767864
[162] loss: 1166.8767864
[163] loss: 1166.8767864
[164] loss: 1166.8767864
[165] loss: 1166.8767864
[166] loss: 1166.8767864
[167] loss: 1166.8767864
[168] loss: 1166.8767864
[169] loss: 1166.8767864
[170] loss: 1166.8767864
[171] loss: 1166.8767864
[172] loss: 1166.8767864
[173] loss: 1166.8767864
[174] loss: 1166.8767864
[175] loss: 1166.8767864
[176] loss: 1166.8767864
[177] loss: 1166.8767864
[178] loss: 1166.8767864
[179] loss: 1166.8767864
[180] loss: 1166.8767864
[181] loss: 1166.8767864
[182] loss: 1166.8767864
[183] loss: 1166.8767864
[184] loss: 1166.8767864
[185] loss: 1166.8767864
[186] loss: 1166.8767864
[187] loss: 1166.8767864
[188] loss: 1166.8767864
[189] loss: 1166.8767864
[190] loss: 1166.8767864
[191] loss: 1166.8767864
[192] loss: 1166.8767864
[193] loss: 1166.8767864
[194] loss: 1166.8767864
[195] loss: 1166.8767864
[196] loss: 1166.8767864
[197] loss: 1166.8767864
[198] loss: 1166.8767864
[199] loss: 1166.8767864
[200] loss: 1166.8767864
[201] loss: 1166.8767864
[202] loss: 1166.8767864
[203] loss: 1166.8767864
[204] loss: 1166.8767864
[205] loss: 1166.8767864
[206] loss: 1166.8767864
[207] loss: 1166.8767864
[208] loss: 1166.8767864
[209] loss: 1166.8767864
Finished Training
The r2m value for this run is:  nan
The AUPR for this run is:  0.649
The Concordance Index (CI) for this run is:  0.5
The Mean Squared Error (MSE) for this run is:  42.98
r2m std is:  nan
AUPR std is:  0.0
CIs std is:  0.0
Best r2m was:  0.20706198944736515
Best parameters were: [395, 0.0045000000000000005, 209]


PS:

Read file <stderr_pkd_d1_p1.log> for stderr output of this job.

