Sender: LSF System <lsfadmin@lsf-server-3.rostclust>
Subject: Job 13136: <dg_prediction_pkd_d1_p1> in cluster <rost_lsf_cluster_1> Done

Job <dg_prediction_pkd_d1_p1> was submitted from host <lsf-master-1.rostclust> by user <giessing> in cluster <rost_lsf_cluster_1> at Mon Dec 13 08:51:01 2021
Job was executed on host(s) <lsf-server-3.rostclust>, in queue <low-end-normal>, as user <giessing> in cluster <rost_lsf_cluster_1> at Mon Dec 13 08:51:02 2021
</mnt/home/giessing> was used as the home directory.
</mnt/project/protdrugaffinity/dg/binding-affinity-prediction/src> was used as the working directory.
Started at Mon Dec 13 08:51:02 2021
Terminated at Tue Dec 14 05:34:25 2021
Results reported at Tue Dec 14 05:34:25 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python binding_prediction.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   190560.41 sec.
    Max Memory :                                 1707 MB
    Average Memory :                             1643.05 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                26
    Run time :                                   74602 sec.
    Turnaround time :                            74604 sec.

The output (if any) follows:

(['pkd', 'Davis'], 'chemVAE', <Section: INPUT FILES>, True, 1, True, True)
Using device: cuda
[855, 0.002, 160]
[855, 0.002, 160]
[855, 0.002, 160]
[855, 0.002, 160]
[855, 0.002, 160]
[855, 0.002, 160]
[855, 0.002, 160]
[1010, 0.0005, 250]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
[360, 0.0001, 167]
Finished Tuning
0.2133051369831278
[360, 0.0001, 167]
[1] loss: 589.2715515
[2] loss: 81.3071792
[3] loss: 66.7337345
[4] loss: 63.4082468
[5] loss: 60.8347924
[6] loss: 59.2544911
[7] loss: 57.6096405
[8] loss: 56.2646795
[9] loss: 54.8222426
[10] loss: 53.1627894
[11] loss: 51.8814507
[12] loss: 50.9345596
[13] loss: 49.7263089
[14] loss: 48.4332815
[15] loss: 47.1059741
[16] loss: 46.1475342
[17] loss: 45.2608147
[18] loss: 44.1755997
[19] loss: 43.2476949
[20] loss: 42.6268041
[21] loss: 41.6731482
[22] loss: 40.5641791
[23] loss: 40.0477821
[24] loss: 39.0464335
[25] loss: 38.3263067
[26] loss: 37.5621117
[27] loss: 36.8816727
[28] loss: 36.2441896
[29] loss: 35.7656428
[30] loss: 35.0047209
[31] loss: 34.4746326
[32] loss: 33.8688966
[33] loss: 33.2800906
[34] loss: 32.7096172
[35] loss: 32.1181856
[36] loss: 31.7582264
[37] loss: 31.5014588
[38] loss: 31.1346656
[39] loss: 30.5514689
[40] loss: 30.0560221
[41] loss: 29.5872814
[42] loss: 29.0367843
[43] loss: 28.9763912
[44] loss: 28.5845688
[45] loss: 28.1233362
[46] loss: 27.6435517
[47] loss: 27.6922141
[48] loss: 27.4155221
[49] loss: 27.0280018
[50] loss: 26.9509903
[51] loss: 26.2116661
[52] loss: 26.4224925
[53] loss: 25.8054115
[54] loss: 25.9872646
[55] loss: 25.2559350
[56] loss: 25.2670242
[57] loss: 25.2075929
[58] loss: 24.6976132
[59] loss: 24.7831030
[60] loss: 24.5203490
[61] loss: 24.2661996
[62] loss: 24.0337819
[63] loss: 23.9010389
[64] loss: 23.8630216
[65] loss: 23.8327431
[66] loss: 23.5688678
[67] loss: 23.7843632
[68] loss: 23.2358783
[69] loss: 22.9708909
[70] loss: 23.2367634
[71] loss: 22.8040754
[72] loss: 22.8175267
[73] loss: 22.8708477
[74] loss: 23.0395523
[75] loss: 22.6661134
[76] loss: 22.7855304
[77] loss: 23.0282499
[78] loss: 23.2891627
[79] loss: 23.4206241
[80] loss: 24.1194400
[81] loss: 24.8832477
[82] loss: 24.9671797
[83] loss: 22.4261388
[84] loss: 21.5337108
[85] loss: 21.3476941
[86] loss: 20.7492603
[87] loss: 20.6227105
[88] loss: 20.5088655
[89] loss: 20.8933295
[90] loss: 20.5145486
[91] loss: 20.4647347
[92] loss: 20.2017588
[93] loss: 19.9848768
[94] loss: 20.2124303
[95] loss: 20.1528179
[96] loss: 20.1165720
[97] loss: 20.0074054
[98] loss: 19.7867255
[99] loss: 19.8662041
[100] loss: 19.5416567
[101] loss: 19.5256680
[102] loss: 19.4050516
[103] loss: 19.3871661
[104] loss: 19.3980459
[105] loss: 19.2401666
[106] loss: 19.2584373
[107] loss: 19.0625825
[108] loss: 19.1462090
[109] loss: 19.1367128
[110] loss: 19.1613549
[111] loss: 18.9452084
[112] loss: 18.8465729
[113] loss: 18.9200644
[114] loss: 18.8699173
[115] loss: 18.7067382
[116] loss: 18.8367697
[117] loss: 18.4873468
[118] loss: 18.6747361
[119] loss: 18.6065847
[120] loss: 18.6268614
[121] loss: 18.5386499
[122] loss: 18.4379935
[123] loss: 18.4020561
[124] loss: 18.1823345
[125] loss: 18.4360813
[126] loss: 18.1969224
[127] loss: 18.3744584
[128] loss: 18.2052888
[129] loss: 18.2054632
[130] loss: 17.9647853
[131] loss: 18.1952563
[132] loss: 17.8745271
[133] loss: 17.9519258
[134] loss: 17.7044103
[135] loss: 18.0998979
[136] loss: 18.0388252
[137] loss: 18.2232518
[138] loss: 17.9686694
[139] loss: 18.0197080
[140] loss: 18.2020903
[141] loss: 18.5351162
[142] loss: 18.4716544
[143] loss: 17.5990712
[144] loss: 17.5754737
[145] loss: 17.4759563
[146] loss: 17.4236895
[147] loss: 17.3762555
[148] loss: 17.1398384
[149] loss: 17.4653390
[150] loss: 17.6461843
[151] loss: 17.7430787
[152] loss: 17.2553866
[153] loss: 17.4551044
[154] loss: 17.2657317
[155] loss: 17.3396847
[156] loss: 17.1132647
[157] loss: 17.3041517
[158] loss: 17.6448762
[159] loss: 17.1749519
[160] loss: 17.3507585
[161] loss: 17.3941534
[162] loss: 17.8539941
[163] loss: 18.0325069
[164] loss: 18.9776631
[165] loss: 20.1423378
[166] loss: 22.2288654
[167] loss: 22.9925678
Finished Training
The r2m value for this run is:  0.224
The AUPR for this run is:  0.676
The Concordance Index (CI) for this run is:  0.675
The Mean Squared Error (MSE) for this run is:  1.574
r2m std is:  0.008
AUPR std is:  0.008
CIs std is:  0.003
Best r2m was:  0.2133051369831278
Best parameters were: [360, 0.0001, 167]


PS:

Read file <stderr_pkd_d1_p1.log> for stderr output of this job.

