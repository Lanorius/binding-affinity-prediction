Sender: LSF System <lsfadmin@lsf-server-3.rostclust>
Subject: Job 14121: <dg_prediction_davis_overtrain> in cluster <rost_lsf_cluster_1> Done

Job <dg_prediction_davis_overtrain> was submitted from host <lsf-master-1.rostclust> by user <giessing> in cluster <rost_lsf_cluster_1> at Tue Jan 25 09:02:36 2022
Job was executed on host(s) <lsf-server-3.rostclust>, in queue <low-end-normal>, as user <giessing> in cluster <rost_lsf_cluster_1> at Tue Jan 25 09:02:37 2022
</mnt/home/giessing> was used as the home directory.
</mnt/project/protdrugaffinity/dg/binding-affinity-prediction/src> was used as the working directory.
Started at Tue Jan 25 09:02:37 2022
Terminated at Tue Jan 25 10:08:03 2022
Results reported at Tue Jan 25 10:08:03 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python binding_prediction.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   14667.88 sec.
    Max Memory :                                 1687 MB
    Average Memory :                             1639.67 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                26
    Run time :                                   3926 sec.
    Turnaround time :                            3927 sec.

The output (if any) follows:

Using device: cuda
Overtraining, skipped Tuning
Training attempt No: 1
[1] loss: 13.1601296
[2] loss: 12.9088059
[3] loss: 12.6871308
[4] loss: 12.4473112
[5] loss: 12.3143891
[6] loss: 12.0317374
[7] loss: 11.9295394
[8] loss: 11.8556249
[9] loss: 11.7139538
[10] loss: 11.5978709
[11] loss: 11.4176923
[12] loss: 11.2856628
[13] loss: 11.3124786
[14] loss: 11.0323434
[15] loss: 10.8183262
[16] loss: 10.7458237
[17] loss: 10.6350263
[18] loss: 10.7077907
[19] loss: 10.5912619
[20] loss: 10.3649772
[21] loss: 10.4510148
[22] loss: 10.3554894
[23] loss: 10.2510774
[24] loss: 10.2805325
[25] loss: 10.0699332
[26] loss: 10.1623688
[27] loss: 9.9658867
[28] loss: 9.8763397
[29] loss: 9.8940936
[30] loss: 9.9752618
[31] loss: 9.7847304
[32] loss: 9.6562776
[33] loss: 9.6873810
[34] loss: 9.7344290
[35] loss: 9.6856310
[36] loss: 9.7671595
[37] loss: 9.3969743
[38] loss: 9.3780630
[39] loss: 9.2168141
[40] loss: 9.1547620
[41] loss: 9.0253994
[42] loss: 9.1691329
[43] loss: 8.9661050
[44] loss: 8.9109063
[45] loss: 8.8143704
[46] loss: 8.5363572
[47] loss: 8.6654334
[48] loss: 8.6903363
[49] loss: 8.5877411
[50] loss: 8.6192068
[51] loss: 8.5763332
[52] loss: 8.3336833
[53] loss: 8.2137271
[54] loss: 8.1792171
[55] loss: 8.3123375
[56] loss: 8.3632765
[57] loss: 8.2879064
[58] loss: 8.1732475
[59] loss: 8.2334077
[60] loss: 8.1957700
[61] loss: 8.1126295
[62] loss: 8.1770612
[63] loss: 7.9147693
[64] loss: 7.9843275
[65] loss: 7.9557869
[66] loss: 7.9185763
[67] loss: 8.0053143
[68] loss: 8.0180535
[69] loss: 7.8996249
[70] loss: 7.8157707
[71] loss: 7.7951734
[72] loss: 7.7945856
[73] loss: 7.9011344
[74] loss: 7.7078534
[75] loss: 7.6745815
[76] loss: 7.6197189
[77] loss: 7.8199597
[78] loss: 7.6720134
[79] loss: 7.5186903
[80] loss: 7.5785319
[81] loss: 7.4547870
[82] loss: 7.5330289
[83] loss: 7.3756108
[84] loss: 7.4341765
[85] loss: 7.3451335
[86] loss: 7.3685129
[87] loss: 7.3412162
[88] loss: 7.2375576
[89] loss: 7.2244709
[90] loss: 7.2472030
[91] loss: 7.2723544
[92] loss: 7.1272032
[93] loss: 6.9464238
[94] loss: 7.2155788
[95] loss: 7.2400432
[96] loss: 7.0854287
[97] loss: 7.1828711
[98] loss: 6.9704316
[99] loss: 7.1431530
[100] loss: 7.2343454
[101] loss: 7.0355911
[102] loss: 7.0912109
[103] loss: 6.9653577
[104] loss: 6.9241265
[105] loss: 7.0739982
[106] loss: 6.9914545
[107] loss: 6.9517052
[108] loss: 7.0945139
[109] loss: 6.8906037
[110] loss: 6.9486641
[111] loss: 6.9448669
[112] loss: 6.8603736
[113] loss: 7.0346329
[114] loss: 6.9170812
[115] loss: 6.8863494
[116] loss: 6.7374223
[117] loss: 6.7724583
[118] loss: 6.8743999
[119] loss: 6.9724344
[120] loss: 6.7852523
[121] loss: 6.7143897
[122] loss: 6.7478410
[123] loss: 6.6280399
[124] loss: 6.9307736
[125] loss: 6.6392140
[126] loss: 6.7034250
[127] loss: 6.7174132
[128] loss: 6.7898708
[129] loss: 6.7418614
[130] loss: 6.5728055
[131] loss: 6.8112077
[132] loss: 6.7081196
[133] loss: 6.6676981
[134] loss: 6.6240351
[135] loss: 6.6992110
[136] loss: 6.4734294
[137] loss: 6.5385353
[138] loss: 6.5468728
[139] loss: 6.6073723
[140] loss: 6.5342616
[141] loss: 6.5083213
[142] loss: 6.5830945
[143] loss: 6.5366598
[144] loss: 6.4892437
[145] loss: 6.5155017
[146] loss: 6.6073989
[147] loss: 6.4356930
[148] loss: 6.4271605
[149] loss: 6.6549696
[150] loss: 6.6555406
[151] loss: 6.5460205
[152] loss: 6.4394897
[153] loss: 6.3750750
[154] loss: 6.4066106
[155] loss: 6.4615996
[156] loss: 6.4824820
[157] loss: 6.4040520
[158] loss: 6.4528526
[159] loss: 6.4092679
[160] loss: 6.3942299
[161] loss: 6.4220802
[162] loss: 6.4372507
[163] loss: 6.4037020
[164] loss: 6.4532840
[165] loss: 6.4273583
[166] loss: 6.5204596
[167] loss: 6.4265359
[168] loss: 6.3245322
[169] loss: 6.2378447
[170] loss: 6.3670249
[171] loss: 6.4681338
[172] loss: 6.2489521
[173] loss: 6.3118162
[174] loss: 6.3030050
[175] loss: 6.3934183
[176] loss: 6.2957959
[177] loss: 6.3461587
[178] loss: 6.3025980
[179] loss: 6.2178848
[180] loss: 6.3327866
[181] loss: 6.3066861
[182] loss: 6.2493694
[183] loss: 6.2064884
[184] loss: 6.2974640
[185] loss: 6.3436658
[186] loss: 6.2357750
[187] loss: 6.1377007
[188] loss: 6.1572890
[189] loss: 6.1585313
[190] loss: 6.2392151
[191] loss: 6.2383624
[192] loss: 6.2034710
[193] loss: 6.1077146
[194] loss: 6.1293323
[195] loss: 6.2816071
[196] loss: 6.1931191
[197] loss: 6.1350795
[198] loss: 6.1106103
[199] loss: 6.2005214
[200] loss: 6.0927886
[201] loss: 6.2149065
[202] loss: 6.0567081
[203] loss: 6.1629822
[204] loss: 6.0280654
[205] loss: 6.1636143
[206] loss: 6.1840489
[207] loss: 6.0539984
[208] loss: 6.1310235
[209] loss: 6.0608288
[210] loss: 6.0946221
[211] loss: 6.1620762
[212] loss: 6.1331768
[213] loss: 6.0924660
[214] loss: 6.2563555
[215] loss: 6.0733021
[216] loss: 6.2541455
[217] loss: 5.9827819
[218] loss: 6.1211915
[219] loss: 6.2026512
[220] loss: 6.3398090
[221] loss: 6.1827462
[222] loss: 6.0304958
[223] loss: 6.2959885
[224] loss: 6.2674931
[225] loss: 6.0262371
[226] loss: 6.0932207
[227] loss: 6.0454652
[228] loss: 6.1342368
[229] loss: 6.0895301
[230] loss: 6.0209587
[231] loss: 5.9791288
[232] loss: 6.0399870
[233] loss: 6.1315901
[234] loss: 6.0538713
[235] loss: 6.1245032
[236] loss: 5.8850197
[237] loss: 6.0658879
[238] loss: 6.0458488
[239] loss: 6.0819597
[240] loss: 6.1308327
[241] loss: 5.9358641
[242] loss: 5.9752837
[243] loss: 5.9618868
[244] loss: 6.0142164
[245] loss: 5.8939179
[246] loss: 6.0193326
[247] loss: 5.9835651
[248] loss: 6.0068208
[249] loss: 5.8625712
[250] loss: 6.0528394
[251] loss: 5.9708248
[252] loss: 5.8556268
[253] loss: 5.8454203
[254] loss: 5.9290812
[255] loss: 5.8652080
[256] loss: 6.0417965
[257] loss: 5.7865979
[258] loss: 5.6893454
[259] loss: 5.8967519
[260] loss: 5.9194189
[261] loss: 5.8546702
[262] loss: 5.9363863
[263] loss: 5.7824893
[264] loss: 5.8481189
[265] loss: 5.9307879
[266] loss: 5.7927352
[267] loss: 5.9341373
[268] loss: 5.9569070
[269] loss: 5.8533503
[270] loss: 5.8509249
[271] loss: 5.6814261
[272] loss: 5.7118972
[273] loss: 5.8164649
[274] loss: 5.6488279
[275] loss: 5.7602673
[276] loss: 5.8635916
[277] loss: 5.7781181
[278] loss: 5.7469296
[279] loss: 5.7701948
[280] loss: 5.7845999
[281] loss: 5.6812468
[282] loss: 5.9102820
[283] loss: 5.6317083
[284] loss: 5.8001227
[285] loss: 5.7247492
[286] loss: 5.7681063
[287] loss: 5.6771314
[288] loss: 5.8249707
[289] loss: 5.7513175
[290] loss: 5.6473652
[291] loss: 5.5806238
[292] loss: 5.7487911
[293] loss: 5.6111191
[294] loss: 5.9192510
[295] loss: 5.8303061
[296] loss: 5.7182974
[297] loss: 5.6523892
[298] loss: 5.7395517
[299] loss: 5.5643249
[300] loss: 5.6402638
[301] loss: 5.7603082
[302] loss: 5.6757152
[303] loss: 5.6168294
[304] loss: 5.5240481
[305] loss: 5.4809270
[306] loss: 5.6065610
[307] loss: 5.6714974
[308] loss: 5.7594616
[309] loss: 5.6669983
[310] loss: 5.5749324
[311] loss: 5.5412751
[312] loss: 5.6709355
[313] loss: 5.5578616
[314] loss: 5.6383934
[315] loss: 5.3786257
[316] loss: 5.5259588
[317] loss: 5.5572745
[318] loss: 5.5135713
[319] loss: 5.5293838
[320] loss: 5.6479339
[321] loss: 5.6019493
[322] loss: 5.5588345
[323] loss: 5.6913738
[324] loss: 5.5168358
[325] loss: 5.4997459
[326] loss: 5.5317363
[327] loss: 5.4740694
[328] loss: 5.2992497
[329] loss: 5.3711111
[330] loss: 5.3740299
[331] loss: 5.5153063
[332] loss: 5.4575778
[333] loss: 5.4458179
[334] loss: 5.4083030
[335] loss: 5.4924829
[336] loss: 5.6130200
[337] loss: 5.4891337
[338] loss: 5.3709214
[339] loss: 5.4583894
[340] loss: 5.4067116
[341] loss: 5.3853464
[342] loss: 5.3466835
[343] loss: 5.4716522
[344] loss: 5.3173149
[345] loss: 5.3448592
[346] loss: 5.3780720
[347] loss: 5.3940537
[348] loss: 5.5365085
[349] loss: 5.2681420
[350] loss: 5.4925216
[351] loss: 5.2920714
[352] loss: 5.1795319
[353] loss: 5.4768636
[354] loss: 5.5197299
[355] loss: 5.3622856
[356] loss: 5.4654243
[357] loss: 5.4884413
[358] loss: 5.4622293
[359] loss: 5.2607086
[360] loss: 5.3309891
[361] loss: 5.3237692
[362] loss: 5.3323622
[363] loss: 5.4545570
[364] loss: 5.2708703
[365] loss: 5.3260254
[366] loss: 5.3598989
[367] loss: 5.4358965
[368] loss: 5.3789140
[369] loss: 5.2327280
[370] loss: 5.3429327
[371] loss: 5.3676305
[372] loss: 5.3947334
[373] loss: 5.3092107
[374] loss: 5.4461205
[375] loss: 5.3354686
[376] loss: 5.2787825
[377] loss: 5.3623791
[378] loss: 5.3877001
[379] loss: 5.3220588
[380] loss: 5.4276568
[381] loss: 5.3780049
[382] loss: 5.3462181
[383] loss: 5.2470175
[384] loss: 5.4076269
[385] loss: 5.1953852
[386] loss: 5.3640842
[387] loss: 5.3817702
[388] loss: 5.2193539
[389] loss: 5.1979836
[390] loss: 5.0648547
[391] loss: 5.4145692
[392] loss: 5.2266069
[393] loss: 5.2304321
[394] loss: 5.2344915
[395] loss: 5.2648385
[396] loss: 5.2420914
[397] loss: 5.2717751
[398] loss: 5.2035156
[399] loss: 5.1928609
[400] loss: 5.0878184
[401] loss: 5.0882143
[402] loss: 5.1183028
[403] loss: 5.3461492
[404] loss: 5.2961393
[405] loss: 5.2234799
[406] loss: 5.2637759
[407] loss: 5.3533726
[408] loss: 5.2060158
[409] loss: 5.3591469
[410] loss: 5.2471643
[411] loss: 5.2208241
[412] loss: 5.2684943
[413] loss: 5.3813057
[414] loss: 5.2051827
[415] loss: 5.1788444
[416] loss: 5.0696947
[417] loss: 5.2547008
[418] loss: 5.2084890
[419] loss: 5.2383536
[420] loss: 5.1753124
[421] loss: 5.2305410
[422] loss: 5.2472306
[423] loss: 5.0867159
[424] loss: 5.1765416
[425] loss: 5.2418525
[426] loss: 5.1908128
[427] loss: 5.2852836
[428] loss: 5.3166030
[429] loss: 5.0182176
[430] loss: 5.1914096
[431] loss: 5.1953928
[432] loss: 5.1790140
[433] loss: 5.1510166
[434] loss: 5.2024571
[435] loss: 5.1167523
[436] loss: 5.1516810
[437] loss: 5.2985540
[438] loss: 5.0763030
[439] loss: 5.0503747
[440] loss: 5.3516536
[441] loss: 5.0044888
[442] loss: 5.2252922
[443] loss: 5.1503826
[444] loss: 5.2139821
[445] loss: 5.2280131
[446] loss: 5.2370128
[447] loss: 5.1181608
[448] loss: 5.2335006
[449] loss: 5.3241697
[450] loss: 5.2489563
[451] loss: 5.2955913
[452] loss: 5.1946448
[453] loss: 5.2905750
[454] loss: 5.2513246
[455] loss: 5.2234242
[456] loss: 5.2300084
[457] loss: 5.3062399
[458] loss: 5.2023732
[459] loss: 5.1238916
[460] loss: 5.1964424
[461] loss: 5.1493251
[462] loss: 5.3429746
[463] loss: 5.0700278
[464] loss: 5.3191832
[465] loss: 5.2176684
[466] loss: 5.0787941
[467] loss: 5.2109304
[468] loss: 5.1967519
[469] loss: 5.3717382
[470] loss: 5.2780537
[471] loss: 5.2800537
[472] loss: 5.1332125
[473] loss: 5.3957502
[474] loss: 5.2136172
[475] loss: 5.2262492
[476] loss: 5.3242660
[477] loss: 5.1560458
[478] loss: 5.2721834
[479] loss: 5.1965465
[480] loss: 5.2250979
[481] loss: 5.2414746
[482] loss: 5.1559863
[483] loss: 5.2590673
[484] loss: 5.2152315
[485] loss: 5.2624299
[486] loss: 5.1377415
[487] loss: 5.2968509
[488] loss: 5.1442688
[489] loss: 5.2224033
[490] loss: 5.3433410
[491] loss: 5.1671286
[492] loss: 5.2129207
[493] loss: 5.3130038
[494] loss: 5.1101742
[495] loss: 5.1953631
[496] loss: 5.0804442
[497] loss: 5.1805114
[498] loss: 5.0397988
[499] loss: 5.1704021
[500] loss: 5.1412854
[501] loss: 5.0971371
[502] loss: 5.0335296
[503] loss: 5.0740929
[504] loss: 5.1667722
[505] loss: 5.2839513
[506] loss: 5.0879725
[507] loss: 5.1543347
[508] loss: 5.1057284
[509] loss: 5.2089878
[510] loss: 5.1537009
[511] loss: 5.1153056
[512] loss: 5.2187236
[513] loss: 5.0997225
[514] loss: 5.2191257
[515] loss: 5.2682897
[516] loss: 5.2215065
[517] loss: 5.0358948
[518] loss: 5.0497303
[519] loss: 5.2089198
[520] loss: 5.1843253
[521] loss: 5.1005247
[522] loss: 5.1013378
[523] loss: 5.1466909
[524] loss: 5.2055670
[525] loss: 5.2259761
[526] loss: 5.0019673
[527] loss: 5.2485915
[528] loss: 5.2424970
[529] loss: 5.2473729
[530] loss: 5.1253009
[531] loss: 5.1770600
[532] loss: 5.1865765
[533] loss: 5.1129684
[534] loss: 5.1888978
[535] loss: 5.0988607
[536] loss: 5.2658597
[537] loss: 5.0899239
[538] loss: 5.2127452
[539] loss: 5.2504891
[540] loss: 5.1759755
[541] loss: 5.2661332
[542] loss: 5.1396395
[543] loss: 5.1569520
[544] loss: 5.2170957
[545] loss: 5.2337635
[546] loss: 5.4040372
[547] loss: 5.3128667
[548] loss: 5.1365242
[549] loss: 5.3238381
[550] loss: 5.2776948
[551] loss: 5.2793300
[552] loss: 5.2237602
[553] loss: 5.2043208
[554] loss: 5.3078498
[555] loss: 5.1896586
[556] loss: 5.1058671
[557] loss: 5.2168930
[558] loss: 5.0806245
[559] loss: 5.1966147
[560] loss: 5.3122857
[561] loss: 5.1666245
[562] loss: 5.3313856
[563] loss: 5.2584451
[564] loss: 5.2610303
[565] loss: 5.1810226
[566] loss: 5.2723061
[567] loss: 5.2545607
Finished Training
The r2m value for this run is:  0.652
The AUPR for this run is:  0.722
The Concordance Index (CI) for this run is:  0.878
The Mean Squared Error (MSE) for this run is:  0.246
r2m std is:  0.012
AUPR std is:  0.016
CI std is:  0.003
MSE std is:  0.006
Best parameters were: [216, 0.0001, 567]


PS:

Read file <stderr_davis_overtrain.log> for stderr output of this job.

