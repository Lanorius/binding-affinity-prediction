Sender: LSF System <lsfadmin@lsf-server-3.rostclust>
Subject: Job 13814: <dg_prediction_davis_overtrain> in cluster <rost_lsf_cluster_1> Done

Job <dg_prediction_davis_overtrain> was submitted from host <lsf-master-1.rostclust> by user <giessing> in cluster <rost_lsf_cluster_1> at Sun Jan 16 09:40:52 2022
Job was executed on host(s) <lsf-server-3.rostclust>, in queue <low-end-normal>, as user <giessing> in cluster <rost_lsf_cluster_1> at Sun Jan 16 09:40:53 2022
</mnt/home/giessing> was used as the home directory.
</mnt/project/protdrugaffinity/dg/binding-affinity-prediction/src> was used as the working directory.
Started at Sun Jan 16 09:40:53 2022
Terminated at Sun Jan 16 17:08:13 2022
Results reported at Sun Jan 16 17:08:13 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python binding_prediction.py
------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   63407.47 sec.
    Max Memory :                                 1674 MB
    Average Memory :                             1661.15 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              6
    Max Threads :                                26
    Run time :                                   26840 sec.
    Turnaround time :                            26841 sec.

The output (if any) follows:

Using device: cuda
[193, 0.0001, 382]
[193, 0.0001, 382]
[193, 0.0001, 382]
[193, 0.0001, 382]
[193, 0.0001, 382]
[193, 0.0001, 382]
[193, 0.0001, 382]
[193, 0.0001, 382]
[193, 0.0001, 382]
[193, 0.0001, 382]
Finished Tuning
0.5789258112028296
[193, 0.0001, 382]
Training attempt No: 1
[1] loss: 371.1549148
[2] loss: 70.8729733
[3] loss: 68.2513860
[4] loss: 66.5133741
[5] loss: 65.5066615
[6] loss: 64.7996176
[7] loss: 63.9184275
[8] loss: 63.2558223
[9] loss: 63.0805469
[10] loss: 62.4881526
[11] loss: 61.7901510
[12] loss: 61.5247839
[13] loss: 60.5353107
[14] loss: 59.6320508
[15] loss: 58.5784937
[16] loss: 57.7503539
[17] loss: 57.1084005
[18] loss: 56.7848024
[19] loss: 56.3797332
[20] loss: 56.0179102
[21] loss: 55.2979432
[22] loss: 54.9237513
[23] loss: 54.5807312
[24] loss: 54.5757097
[25] loss: 54.1708789
[26] loss: 54.1630621
[27] loss: 53.4892953
[28] loss: 52.9263238
[29] loss: 53.1628089
[30] loss: 52.4236076
[31] loss: 51.8868596
[32] loss: 51.4893813
[33] loss: 51.0212719
[34] loss: 50.2885821
[35] loss: 49.9430711
[36] loss: 49.3626745
[37] loss: 49.2887101
[38] loss: 48.2546970
[39] loss: 48.0089490
[40] loss: 47.7300576
[41] loss: 47.6035877
[42] loss: 46.7070970
[43] loss: 46.3962201
[44] loss: 46.3267298
[45] loss: 45.8229862
[46] loss: 45.3546016
[47] loss: 45.3901959
[48] loss: 45.0596319
[49] loss: 44.5807063
[50] loss: 44.3176267
[51] loss: 44.2386466
[52] loss: 43.9553989
[53] loss: 43.9883545
[54] loss: 43.2744470
[55] loss: 43.3507405
[56] loss: 43.3515574
[57] loss: 42.7918912
[58] loss: 42.7088401
[59] loss: 42.5364799
[60] loss: 42.2766741
[61] loss: 42.4276377
[62] loss: 41.9937359
[63] loss: 41.6691014
[64] loss: 41.6286335
[65] loss: 41.3325369
[66] loss: 41.1225889
[67] loss: 40.9859084
[68] loss: 40.6349467
[69] loss: 40.1319189
[70] loss: 40.6222784
[71] loss: 40.3732791
[72] loss: 40.3311644
[73] loss: 39.9633700
[74] loss: 39.5306896
[75] loss: 39.5326779
[76] loss: 39.4880059
[77] loss: 38.8771509
[78] loss: 39.1708022
[79] loss: 38.9241710
[80] loss: 38.6108980
[81] loss: 38.2605973
[82] loss: 38.1553664
[83] loss: 37.9242983
[84] loss: 38.3638113
[85] loss: 37.8338163
[86] loss: 37.4018013
[87] loss: 37.6985579
[88] loss: 37.3024429
[89] loss: 37.0846778
[90] loss: 36.9408832
[91] loss: 37.0871478
[92] loss: 37.0129987
[93] loss: 36.7409942
[94] loss: 36.7962259
[95] loss: 36.7068195
[96] loss: 36.1932087
[97] loss: 36.2337034
[98] loss: 35.9577899
[99] loss: 36.4970971
[100] loss: 35.9971646
[101] loss: 35.9039405
[102] loss: 35.5560358
[103] loss: 35.7339365
[104] loss: 35.6619495
[105] loss: 35.5776781
[106] loss: 35.4577626
[107] loss: 35.2277895
[108] loss: 35.3018896
[109] loss: 35.1673278
[110] loss: 34.9421130
[111] loss: 34.9744888
[112] loss: 34.6003461
[113] loss: 34.9956206
[114] loss: 34.8231528
[115] loss: 34.6993742
[116] loss: 34.7341074
[117] loss: 34.4091684
[118] loss: 34.3706676
[119] loss: 34.0130157
[120] loss: 33.9833674
[121] loss: 33.8798470
[122] loss: 33.9655182
[123] loss: 33.6537151
[124] loss: 33.7948078
[125] loss: 33.7383493
[126] loss: 33.3133069
[127] loss: 33.3992426
[128] loss: 33.3583053
[129] loss: 33.4010112
[130] loss: 32.7804821
[131] loss: 32.8024830
[132] loss: 32.9886226
[133] loss: 32.7855820
[134] loss: 32.8630433
[135] loss: 32.8706830
[136] loss: 32.3510071
[137] loss: 32.7358389
[138] loss: 32.6156403
[139] loss: 32.0772891
[140] loss: 32.2699888
[141] loss: 32.2211807
[142] loss: 31.8019857
[143] loss: 32.1114604
[144] loss: 32.0237463
[145] loss: 31.5987237
[146] loss: 31.5735141
[147] loss: 31.8075787
[148] loss: 31.5380001
[149] loss: 31.2614131
[150] loss: 31.4301318
[151] loss: 31.4279803
[152] loss: 31.5115972
[153] loss: 31.2327395
[154] loss: 31.1555933
[155] loss: 30.9784461
[156] loss: 30.7100531
[157] loss: 30.8102502
[158] loss: 30.9331942
[159] loss: 30.8572874
[160] loss: 30.4851183
[161] loss: 30.7340559
[162] loss: 30.6377335
[163] loss: 30.3498498
[164] loss: 30.2081710
[165] loss: 30.1554733
[166] loss: 29.9803181
[167] loss: 29.9494975
[168] loss: 30.2349457
[169] loss: 30.3257224
[170] loss: 30.0454471
[171] loss: 29.8208578
[172] loss: 29.7309203
[173] loss: 29.7275204
[174] loss: 29.5497350
[175] loss: 29.5347609
[176] loss: 29.2228789
[177] loss: 29.5026314
[178] loss: 29.4864143
[179] loss: 29.1488070
[180] loss: 29.1782376
[181] loss: 29.0417278
[182] loss: 29.2097698
[183] loss: 28.7244691
[184] loss: 29.1499470
[185] loss: 28.8429092
[186] loss: 28.7335405
[187] loss: 28.3636758
[188] loss: 28.4199336
[189] loss: 28.2948195
[190] loss: 28.5037160
[191] loss: 28.3869945
[192] loss: 28.2446879
[193] loss: 28.3265091
[194] loss: 28.0562236
[195] loss: 28.1018511
[196] loss: 28.1291233
[197] loss: 27.9905501
[198] loss: 27.7758844
[199] loss: 27.4818130
[200] loss: 27.8112556
[201] loss: 27.3896205
[202] loss: 27.5455915
[203] loss: 27.3850178
[204] loss: 27.1652106
[205] loss: 27.2244916
[206] loss: 27.1485497
[207] loss: 26.7807558
[208] loss: 26.9002541
[209] loss: 26.8864250
[210] loss: 26.9036975
[211] loss: 26.6945218
[212] loss: 26.8208610
[213] loss: 26.6662098
[214] loss: 26.5761120
[215] loss: 26.6372683
[216] loss: 26.5752032
[217] loss: 26.4785703
[218] loss: 26.2489598
[219] loss: 26.4680883
[220] loss: 26.4280900
[221] loss: 26.3256065
[222] loss: 26.1829647
[223] loss: 25.9624974
[224] loss: 26.0418215
[225] loss: 25.7575094
[226] loss: 25.4914065
[227] loss: 25.3459662
[228] loss: 25.9362084
[229] loss: 25.6727792
[230] loss: 25.3549737
[231] loss: 25.7833161
[232] loss: 25.2916005
[233] loss: 25.3405529
[234] loss: 25.1178246
[235] loss: 25.1870630
[236] loss: 25.4726958
[237] loss: 25.1291081
[238] loss: 25.0693768
[239] loss: 24.7006725
[240] loss: 24.7991023
[241] loss: 24.7553196
[242] loss: 24.5796590
[243] loss: 24.4567381
[244] loss: 24.8054109
[245] loss: 24.4925020
[246] loss: 24.3056007
[247] loss: 24.3350346
[248] loss: 24.2576260
[249] loss: 24.3393544
[250] loss: 24.0982426
[251] loss: 23.8555296
[252] loss: 23.7978279
[253] loss: 23.8291024
[254] loss: 23.6858831
[255] loss: 23.5372317
[256] loss: 23.6061991
[257] loss: 23.5331046
[258] loss: 23.5749448
[259] loss: 23.4957705
[260] loss: 23.3379518
[261] loss: 23.4689059
[262] loss: 22.9665470
[263] loss: 23.1673496
[264] loss: 22.9858068
[265] loss: 22.7785108
[266] loss: 23.1805347
[267] loss: 22.7744977
[268] loss: 22.7936661
[269] loss: 22.7360147
[270] loss: 22.5243126
[271] loss: 22.3608459
[272] loss: 22.1942709
[273] loss: 22.4917047
[274] loss: 22.2830572
[275] loss: 22.3078977
[276] loss: 22.2282871
[277] loss: 21.9879052
[278] loss: 22.0619942
[279] loss: 22.0501504
[280] loss: 21.5687761
[281] loss: 22.0082970
[282] loss: 21.7883620
[283] loss: 21.9428755
[284] loss: 21.7915173
[285] loss: 21.4486263
[286] loss: 21.5561742
[287] loss: 21.2759742
[288] loss: 21.2556668
[289] loss: 21.3287983
[290] loss: 21.2420705
[291] loss: 20.9744000
[292] loss: 20.8692556
[293] loss: 21.1783324
[294] loss: 20.7528009
[295] loss: 20.9739740
[296] loss: 20.5551081
[297] loss: 20.6251555
[298] loss: 20.6263416
[299] loss: 20.4598965
[300] loss: 20.4313563
[301] loss: 20.1467130
[302] loss: 20.0400210
[303] loss: 20.2954658
[304] loss: 20.3675719
[305] loss: 19.9326092
[306] loss: 20.0148431
[307] loss: 19.7682594
[308] loss: 19.7976057
[309] loss: 19.9252567
[310] loss: 19.8306079
[311] loss: 20.0579209
[312] loss: 19.5661639
[313] loss: 19.6810756
[314] loss: 19.2993790
[315] loss: 19.6646028
[316] loss: 19.3097515
[317] loss: 19.4773239
[318] loss: 19.3440630
[319] loss: 19.0881707
[320] loss: 19.0549007
[321] loss: 19.0026356
[322] loss: 18.7679057
[323] loss: 18.9678084
[324] loss: 18.9117929
[325] loss: 18.7322850
[326] loss: 18.4213728
[327] loss: 18.6671167
[328] loss: 18.5027591
[329] loss: 18.3477248
[330] loss: 18.4420874
[331] loss: 18.1536377
[332] loss: 18.3588942
[333] loss: 18.2311936
[334] loss: 18.1622038
[335] loss: 17.8544321
[336] loss: 17.8798462
[337] loss: 17.9187320
[338] loss: 17.8503838
[339] loss: 17.8215204
[340] loss: 17.6450738
[341] loss: 17.6817563
[342] loss: 17.3246386
[343] loss: 17.5708455
[344] loss: 17.6374632
[345] loss: 17.1772840
[346] loss: 17.1653328
[347] loss: 17.1914834
[348] loss: 17.1756385
[349] loss: 17.0897436
[350] loss: 17.0324825
[351] loss: 16.9534493
[352] loss: 16.8240538
[353] loss: 16.7546085
[354] loss: 16.7862394
[355] loss: 16.8321264
[356] loss: 16.7843761
[357] loss: 16.8117180
[358] loss: 16.5372604
[359] loss: 16.2526621
[360] loss: 16.6226028
[361] loss: 16.2252159
[362] loss: 16.3891919
[363] loss: 16.1762390
[364] loss: 15.9323206
[365] loss: 15.9565114
[366] loss: 16.0863793
[367] loss: 15.7798030
[368] loss: 15.8999263
[369] loss: 15.7924876
[370] loss: 15.5600116
[371] loss: 15.4707402
[372] loss: 15.7690354
[373] loss: 15.6930170
[374] loss: 15.5036161
[375] loss: 15.3654679
[376] loss: 15.3683022
[377] loss: 15.2022429
[378] loss: 15.3880565
[379] loss: 15.2781912
[380] loss: 15.0182296
[381] loss: 15.0792962
[382] loss: 15.0438575
Finished Training
The r2m value for this run is:  0.658
The AUPR for this run is:  0.716
The Concordance Index (CI) for this run is:  0.876
The Mean Squared Error (MSE) for this run is:  0.241
r2m std is:  0.012
AUPR std is:  0.016
CIs std is:  0.004
Best r2m was:  0.5789258112028296
Best parameters were: [193, 0.0001, 382]


PS:

Read file <stderr_davis_overtrain.log> for stderr output of this job.

